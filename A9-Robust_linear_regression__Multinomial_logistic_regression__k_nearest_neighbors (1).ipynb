{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7ab2c6c4",
      "metadata": {
        "id": "7ab2c6c4"
      },
      "source": [
        "# MISA\n",
        "Alohan'ny mamerina dia avereno atao Run ny notebook iray manontolo. Ny fanaovana azy dia redémarrena mihitsy ny kernel aloha (jereo menubar, safidio **Kernel$\\rightarrow$Restart Kernel and Run All Cells**).\n",
        "\n",
        "Izay misy hoe `YOUR CODE HERE` na `YOUR ANSWER HERE` ihany no fenoina. Afaka manampy cells vaovao raha ilaina. Aza adino ny mameno references eo ambany raha ilaina."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0416c784",
      "metadata": {
        "id": "0416c784"
      },
      "source": [
        "## References\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9296767",
      "metadata": {
        "id": "e9296767"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "Z7CXFaPKJTZM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7CXFaPKJTZM",
        "outputId": "505e0480-79d9-43ae-f89b-88b7f24c6179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn==1.1.3\n",
            "  Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (3.2.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 0.16.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scikit-learn-1.1.3\n"
          ]
        }
      ],
      "source": [
        "! pip install scikit-learn==1.1.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "aebdce2a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "aebdce2a",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a003bb8fd03260393cb0121b2dcaf283",
          "grade": false,
          "grade_id": "cell-bb34820fb5daebb2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# AZA MANAMPY CODE ATO FA MNAOVA CELLULE VAOVAO\n",
        "\n",
        "from random import randrange\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, log_loss\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.datasets import load_boston, load_diabetes, load_iris, load_digits\n",
        "from scipy.special import huber\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def grad_check_sparse(f, x, analytic_grad, num_checks=12, h=1e-5, error=1e-9):\n",
        "    \"\"\"\n",
        "    sample a few random elements and only return numerical\n",
        "    in this dimensions\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(num_checks):\n",
        "        ix = tuple([randrange(m) for m in x.shape])\n",
        "\n",
        "        oldval = x[ix]\n",
        "        x[ix] = oldval + h  # increment by h\n",
        "        fxph = f(x)  # evaluate f(x + h)\n",
        "        x[ix] = oldval - h  # increment by h\n",
        "        fxmh = f(x)  # evaluate f(x - h)\n",
        "        x[ix] = oldval  # reset\n",
        "\n",
        "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
        "        grad_analytic = analytic_grad[ix]\n",
        "        rel_error = abs(grad_numerical - grad_analytic) / (\n",
        "            abs(grad_numerical) + abs(grad_analytic)\n",
        "        )\n",
        "        print(\n",
        "            \"numerical: %f analytic: %f, relative error: %e\"\n",
        "            % (grad_numerical, grad_analytic, rel_error)\n",
        "        )\n",
        "        assert rel_error < error\n",
        "\n",
        "def rel_error(x, y):\n",
        "    \"\"\" returns relative error \"\"\"\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b72ea596",
      "metadata": {
        "id": "b72ea596"
      },
      "source": [
        "# Robust linear regression - Huber loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9405baa1",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "9405baa1",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c1ef69a259cc98ed9c1a4b75d4675c8d",
          "grade": false,
          "grade_id": "cell-050726493bce8a41",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "data = load_boston()\n",
        "X_train1, y_train1 = data.data, data.target\n",
        "w1 = np.random.randn(X_train1.shape[1]) * 0.0001\n",
        "b1 = np.random.randn(1) * 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "b79b3750",
      "metadata": {
        "deletable": false,
        "id": "b79b3750",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "095fe70b6ce7c4be3afef42fd9756ac5",
          "grade": false,
          "grade_id": "cell-773a9a9798718bb2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def huber_loss_naive(w, b, X, y, epsilon=1.35, alpha=0.0001):\n",
        "    \"\"\"\n",
        "    Huber loss for all observations\n",
        "\n",
        "    Inputs:\n",
        "    - w: array of shape (D,) containing weights\n",
        "    - b: float bias\n",
        "    - X: array of shape (N, D) containing a minibatch of data\n",
        "    - y: array of shape (N,) containing training labels\n",
        "    - epsilon: float\n",
        "    - alpha: regularization\n",
        "    \"\"\"\n",
        "\n",
        "    loss = 0.0\n",
        "    dw = np.zeros_like(w)\n",
        "    db = 0.0\n",
        "\n",
        "    for  i in range(len(y)):\n",
        "        diff = (w.T).dot(X[i])+b-y[i]\n",
        "        if abs(diff) <= epsilon:\n",
        "            loss+=(1/2)*((w.T).dot(X[i])+b - y[i])**2\n",
        "        else:\n",
        "            loss+=epsilon*abs(diff)-(epsilon**2)/2\n",
        "\n",
        "    loss*=1/len(y)\n",
        "\n",
        "    #dw\n",
        "\n",
        "    for k in range (len(w)):\n",
        "        for i in range (len (y)):\n",
        "            diff = (w.T).dot(X[i])+b-y[i]\n",
        "            if abs(diff) <= epsilon:\n",
        "                dw[k] += X[i][k]*((w.T).dot(X[i])+b - y[i])\n",
        "            else :\n",
        "                dw[k] += X[i][k]* epsilon* np.sign(((w.T).dot(X[i])+b - y[i]))\n",
        "    dw *=1/len(y)\n",
        "    #db\n",
        "    for i in range(len(y)):\n",
        "      diff = (w.T).dot(X[i])+b-y[i]\n",
        "      if abs(diff) <= epsilon:\n",
        "        db += diff\n",
        "      else :\n",
        "        db += epsilon* np.sign(((w.T).dot(X[i])+b-y[i]))\n",
        "    db*=1/len(y)\n",
        "\n",
        "    loss+= alpha*(w @ w) + alpha *b*b\n",
        "    dw += 2 * alpha * w\n",
        "    db +=  2 * alpha *b\n",
        "\n",
        "    return loss, dw, np.array(db).reshape(1,)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92ca4921",
      "metadata": {
        "id": "92ca4921"
      },
      "source": [
        "## without regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "7ae434b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "7ae434b9",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2e49c0c5b9c763c872c9eccf3c34e14f",
          "grade": true,
          "grade_id": "cell-88996bb95da97f10",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "777e249a-50f7-4f2d-8c69-e2a5b30f1bc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient check w\n",
            "numerical: -0.748838 analytic: -0.748838, relative error: 5.742450e-10\n",
            "numerical: -4.878257 analytic: -4.878257, relative error: 9.608527e-11\n",
            "numerical: -551.120158 analytic: -551.120158, relative error: 9.650960e-13\n",
            "numerical: -551.120158 analytic: -551.120158, relative error: 9.650960e-13\n",
            "numerical: -12.891700 analytic: -12.891700, relative error: 6.087044e-12\n",
            "numerical: -15.340909 analytic: -15.340909, relative error: 9.718030e-12\n",
            "numerical: -0.748838 analytic: -0.748838, relative error: 5.742450e-10\n",
            "numerical: -481.509943 analytic: -481.509943, relative error: 6.280389e-14\n",
            "numerical: -0.748838 analytic: -0.748838, relative error: 5.742450e-10\n",
            "numerical: -0.748838 analytic: -0.748838, relative error: 5.742450e-10\n",
            "numerical: -551.120158 analytic: -551.120158, relative error: 9.650960e-13\n",
            "numerical: -15.034651 analytic: -15.034651, relative error: 7.298001e-11\n",
            "numerical: -15.340909 analytic: -15.340909, relative error: 9.718030e-12\n",
            "numerical: -551.120158 analytic: -551.120158, relative error: 9.650960e-13\n",
            "numerical: -12.891700 analytic: -12.891700, relative error: 6.087044e-12\n",
            "Gradient check bias\n",
            "numerical: -1.350000 analytic: -1.350000, relative error: 3.413094e-10\n",
            "numerical: -1.350000 analytic: -1.350000, relative error: 3.413094e-10\n",
            "numerical: -1.350000 analytic: -1.350000, relative error: 3.413094e-10\n",
            "numerical: -1.350000 analytic: -1.350000, relative error: 3.413094e-10\n",
            "numerical: -1.350000 analytic: -1.350000, relative error: 3.413094e-10\n",
            "numerical: -1.350000 analytic: -1.350000, relative error: 3.413094e-10\n",
            "numerical: -1.350000 analytic: -1.350000, relative error: 3.413094e-10\n",
            "numerical: -1.350000 analytic: -1.350000, relative error: 3.413094e-10\n",
            "numerical: -1.350000 analytic: -1.350000, relative error: 3.413094e-10\n",
            "numerical: -1.350000 analytic: -1.350000, relative error: 3.413094e-10\n",
            "numerical: -1.350000 analytic: -1.350000, relative error: 3.413094e-10\n",
            "numerical: -1.350000 analytic: -1.350000, relative error: 3.413094e-10\n",
            "numerical: -1.350000 analytic: -1.350000, relative error: 3.413094e-10\n",
            "numerical: -1.350000 analytic: -1.350000, relative error: 3.413094e-10\n",
            "numerical: -1.350000 analytic: -1.350000, relative error: 3.413094e-10\n",
            "Gradient check w large epsilon\n",
            "numerical: -332.348005 analytic: -332.348005, relative error: 2.565623e-12\n",
            "numerical: -8290.524561 analytic: -8290.524561, relative error: 1.454441e-12\n",
            "numerical: -404.549691 analytic: -404.549691, relative error: 1.225068e-11\n",
            "numerical: -145.674273 analytic: -145.674273, relative error: 5.854306e-11\n",
            "numerical: -8290.524561 analytic: -8290.524561, relative error: 1.454441e-12\n",
            "numerical: -8449.099367 analytic: -8449.099367, relative error: 1.219068e-12\n",
            "numerical: -8449.099367 analytic: -8449.099367, relative error: 1.219068e-12\n",
            "numerical: -145.674273 analytic: -145.674273, relative error: 5.854306e-11\n",
            "numerical: -1.962288 analytic: -1.962288, relative error: 7.742977e-10\n",
            "numerical: -12.008686 analytic: -12.008686, relative error: 3.579329e-10\n",
            "numerical: -12.008686 analytic: -12.008686, relative error: 3.579329e-10\n",
            "numerical: -184.159159 analytic: -184.159159, relative error: 6.617183e-11\n",
            "numerical: -184.159159 analytic: -184.159159, relative error: 6.617183e-11\n",
            "numerical: -50.621816 analytic: -50.621816, relative error: 1.170666e-10\n",
            "numerical: -8290.524561 analytic: -8290.524561, relative error: 1.454441e-12\n",
            "Gradient check bias large epsilon\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 5.821587e-10\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 5.821587e-10\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 5.821587e-10\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 5.821587e-10\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 5.821587e-10\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 5.821587e-10\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 5.821587e-10\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 5.821587e-10\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 5.821587e-10\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 5.821587e-10\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 5.821587e-10\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 5.821587e-10\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 5.821587e-10\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 5.821587e-10\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 5.821587e-10\n"
          ]
        }
      ],
      "source": [
        "loss, dw1, db1 = huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)\n",
        "\n",
        "print(\"Gradient check w\")\n",
        "# Check with numerical gradient w\n",
        "f = lambda w1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)[0]\n",
        "grad_numerical = grad_check_sparse(f, w1, dw1, 15, error=1e-9)\n",
        "\n",
        "print(\"Gradient check bias\")\n",
        "# Check with numerical gradient b\n",
        "f2 = lambda b1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)[0]\n",
        "grad_numerical = grad_check_sparse(f2, b1, db1, 15, error=1e-9)\n",
        "\n",
        "\n",
        "# Large epsilon\n",
        "large_eps_loss, large_eps_dw1, large_eps_db1 = huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)\n",
        "\n",
        "print(\"Gradient check w large epsilon\")\n",
        "# Check with numerical gradient w\n",
        "f = lambda w1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)[0]\n",
        "grad_numerical = grad_check_sparse(f, w1, large_eps_dw1, 15, error=1e-9)\n",
        "\n",
        "print(\"Gradient check bias large epsilon\")\n",
        "# Check with numerical gradient b\n",
        "f2 = lambda b1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)[0]\n",
        "grad_numerical = grad_check_sparse(f2, b1, large_eps_db1, 15, error=1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f2b3314",
      "metadata": {
        "id": "9f2b3314"
      },
      "source": [
        " ## with regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "20ed0b8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "20ed0b8d",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "42c6c09b6a3764a91aa10791fd05545e",
          "grade": true,
          "grade_id": "cell-6cfedcc6f02a9770",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "6cf45ef5-56d1-4fe7-dd1b-838ad47c4bfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient check w\n",
            "numerical: -5.123191 analytic: -5.123191, relative error: 9.705827e-11\n",
            "numerical: -15.034639 analytic: -15.034639, relative error: 7.254527e-11\n",
            "numerical: -15.340948 analytic: -15.340948, relative error: 7.864351e-12\n",
            "numerical: -12.891966 analytic: -12.891966, relative error: 4.065638e-12\n",
            "numerical: -0.093181 analytic: -0.093181, relative error: 1.307923e-10\n",
            "numerical: -17.081496 analytic: -17.081496, relative error: 3.050012e-12\n",
            "numerical: -15.034639 analytic: -15.034639, relative error: 7.254527e-11\n",
            "numerical: -15.340948 analytic: -15.340948, relative error: 7.864351e-12\n",
            "numerical: -24.915267 analytic: -24.915267, relative error: 1.953936e-12\n",
            "numerical: -5.123191 analytic: -5.123191, relative error: 9.705827e-11\n",
            "numerical: -17.081496 analytic: -17.081496, relative error: 3.050012e-12\n",
            "numerical: -17.081496 analytic: -17.081496, relative error: 3.050012e-12\n",
            "numerical: -0.093181 analytic: -0.093181, relative error: 1.307923e-10\n",
            "numerical: -92.576131 analytic: -92.576131, relative error: 1.592609e-13\n",
            "numerical: -0.749077 analytic: -0.749077, relative error: 5.386648e-10\n",
            "Gradient check bias\n",
            "numerical: -1.350107 analytic: -1.350107, relative error: 3.240859e-10\n",
            "numerical: -1.350107 analytic: -1.350107, relative error: 3.240859e-10\n",
            "numerical: -1.350107 analytic: -1.350107, relative error: 3.240859e-10\n",
            "numerical: -1.350107 analytic: -1.350107, relative error: 3.240859e-10\n",
            "numerical: -1.350107 analytic: -1.350107, relative error: 3.240859e-10\n",
            "numerical: -1.350107 analytic: -1.350107, relative error: 3.240859e-10\n",
            "numerical: -1.350107 analytic: -1.350107, relative error: 3.240859e-10\n",
            "numerical: -1.350107 analytic: -1.350107, relative error: 3.240859e-10\n",
            "numerical: -1.350107 analytic: -1.350107, relative error: 3.240859e-10\n",
            "numerical: -1.350107 analytic: -1.350107, relative error: 3.240859e-10\n",
            "numerical: -1.350107 analytic: -1.350107, relative error: 3.240859e-10\n",
            "numerical: -1.350107 analytic: -1.350107, relative error: 3.240859e-10\n",
            "numerical: -1.350107 analytic: -1.350107, relative error: 3.240859e-10\n",
            "numerical: -1.350107 analytic: -1.350107, relative error: 3.240859e-10\n",
            "numerical: -1.350107 analytic: -1.350107, relative error: 3.240859e-10\n",
            "Gradient check w large epsilon\n",
            "numerical: -90.073228 analytic: -90.073228, relative error: 1.524915e-10\n",
            "numerical: -8449.099409 analytic: -8449.099409, relative error: 1.285161e-12\n",
            "numerical: -90.073228 analytic: -90.073228, relative error: 1.524915e-10\n",
            "numerical: -332.348044 analytic: -332.348044, relative error: 1.315009e-12\n",
            "numerical: -219.814163 analytic: -219.814163, relative error: 6.720629e-11\n",
            "numerical: -90.073228 analytic: -90.073228, relative error: 1.524915e-10\n",
            "numerical: -404.549988 analytic: -404.549988, relative error: 1.267409e-11\n",
            "numerical: -12.008925 analytic: -12.008925, relative error: 3.675297e-10\n",
            "numerical: -404.549988 analytic: -404.549988, relative error: 1.267409e-11\n",
            "numerical: -8290.524118 analytic: -8290.524118, relative error: 1.406062e-12\n",
            "numerical: -219.814163 analytic: -219.814163, relative error: 6.720629e-11\n",
            "numerical: -235.986078 analytic: -235.986078, relative error: 2.093918e-11\n",
            "numerical: -8449.099409 analytic: -8449.099409, relative error: 1.285161e-12\n",
            "numerical: -219.814163 analytic: -219.814163, relative error: 6.720629e-11\n",
            "numerical: -1443.451157 analytic: -1443.451157, relative error: 5.002390e-12\n",
            "Gradient check bias large epsilon\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 5.871427e-10\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 5.871427e-10\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 5.871427e-10\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 5.871427e-10\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 5.871427e-10\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 5.871427e-10\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 5.871427e-10\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 5.871427e-10\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 5.871427e-10\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 5.871427e-10\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 5.871427e-10\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 5.871427e-10\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 5.871427e-10\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 5.871427e-10\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 5.871427e-10\n"
          ]
        }
      ],
      "source": [
        "loss, dw1, db1 = huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)\n",
        "\n",
        "print(\"Gradient check w\")\n",
        "# Check with numerical gradient w\n",
        "f = lambda w1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)[0]\n",
        "grad_numerical = grad_check_sparse(f, w1, dw1, 15, error=1e-9)\n",
        "\n",
        "print(\"Gradient check bias\")\n",
        "# Check with numerical gradient b\n",
        "f2 = lambda b1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)[0]\n",
        "grad_numerical = grad_check_sparse(f2, b1, db1, 15, error=1e-9)\n",
        "\n",
        "\n",
        "# Large epsilon\n",
        "large_eps_loss, large_eps_dw1, large_eps_db1 = huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)\n",
        "\n",
        "print(\"Gradient check w large epsilon\")\n",
        "# Check with numerical gradient w\n",
        "f = lambda w1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)[0]\n",
        "grad_numerical = grad_check_sparse(f, w1, large_eps_dw1, 15, error=1e-9)\n",
        "\n",
        "print(\"Gradient check bias large epsilon\")\n",
        "# Check with numerical gradient b\n",
        "f2 = lambda b1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)[0]\n",
        "grad_numerical = grad_check_sparse(f2, b1, large_eps_db1, 15, error=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "038cf336",
      "metadata": {
        "deletable": false,
        "id": "038cf336",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6809cf643812e18004a8e019e8648ee7",
          "grade": false,
          "grade_id": "cell-2f0ecc6405bdde00",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def huber_loss_vectorized(w, b, X, y, epsilon=1.35, alpha=0.0001):\n",
        "    \"\"\"\n",
        "    Huber loss for all observations\n",
        "\n",
        "    Inputs:\n",
        "    - w: array of shape (D,) containing weights\n",
        "    - b: float bias\n",
        "    - X: array of shape (N, D) containing a minibatch of data\n",
        "    - y: array of shape (N,) containing training labels\n",
        "    - epsilon: float\n",
        "    - alpha: regularization\n",
        "    \"\"\"\n",
        "    loss = 0.0\n",
        "    dw = np.zeros_like(w)\n",
        "    db = 0\n",
        "\n",
        "\n",
        "    diff= X.dot(w) + b - y\n",
        "\n",
        "    loss = np.where(np.abs(diff), 1/2 * (diff)**2, epsilon*np.abs(diff) - epsilon**2 / 2)\n",
        "    loss = loss.mean()\n",
        "\n",
        "    dloss = np.where(np.abs(diff), diff, epsilon*np.sign(diff))\n",
        "    db = dloss.mean()\n",
        "    dw = X.T.dot(dloss) / X.shape[0]\n",
        "\n",
        "    loss+= alpha*(w @ w) + alpha *b*b\n",
        "    dw += 2 * alpha * w\n",
        "    db +=  2 * alpha *b\n",
        "    return loss, dw, np.array(db).reshape(1,)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "292f866a",
      "metadata": {
        "id": "292f866a"
      },
      "source": [
        "## without regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "ff23bae6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "ff23bae6",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cadca09dc4ee08cf4c73480cfdc14e13",
          "grade": true,
          "grade_id": "cell-cecbe864e0fa149c",
          "locked": true,
          "points": 1.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "5721cb3e-ea56-47f4-c99c-41c976ff4a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient check w\n",
            "numerical: -184.159159 analytic: -184.159159, relative error: 3.277092e-12\n",
            "numerical: -50.621816 analytic: -50.621816, relative error: 3.284914e-11\n",
            "numerical: -50.621816 analytic: -50.621816, relative error: 3.284914e-11\n",
            "numerical: -1443.451143 analytic: -1443.451143, relative error: 5.003650e-13\n",
            "numerical: -332.348005 analytic: -332.348005, relative error: 1.710529e-12\n",
            "numerical: -1.962288 analytic: -1.962288, relative error: 7.742977e-10\n",
            "numerical: -219.814175 analytic: -219.814175, relative error: 4.282376e-12\n",
            "numerical: -12.008686 analytic: -12.008686, relative error: 1.154194e-10\n",
            "numerical: -8290.524561 analytic: -8290.524561, relative error: 2.598862e-13\n",
            "numerical: -1.962288 analytic: -1.962288, relative error: 7.742977e-10\n",
            "numerical: -8449.099367 analytic: -8449.099367, relative error: 4.241173e-14\n",
            "numerical: -184.159159 analytic: -184.159159, relative error: 3.277092e-12\n",
            "numerical: -404.549691 analytic: -404.549691, relative error: 1.800078e-12\n",
            "numerical: -1443.451143 analytic: -1443.451143, relative error: 5.003650e-13\n",
            "numerical: -145.674273 analytic: -145.674273, relative error: 1.121851e-14\n",
            "Gradient check bias\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612588e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612588e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612588e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612588e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612588e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612588e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612588e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612588e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612588e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612588e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612588e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612588e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612588e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612588e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612588e-11\n",
            "Gradient check w large epsilon\n",
            "numerical: -1443.451143 analytic: -1443.451143, relative error: 4.999712e-13\n",
            "numerical: -1.962288 analytic: -1.962288, relative error: 7.742977e-10\n",
            "numerical: -1443.451143 analytic: -1443.451143, relative error: 4.999712e-13\n",
            "numerical: -8449.099367 analytic: -8449.099367, relative error: 4.176587e-14\n",
            "numerical: -145.674273 analytic: -145.674273, relative error: 1.170627e-14\n",
            "numerical: -8290.524561 analytic: -8290.524561, relative error: 2.596668e-13\n",
            "numerical: -50.621816 analytic: -50.621816, relative error: 3.284879e-11\n",
            "numerical: -8449.099367 analytic: -8449.099367, relative error: 4.176587e-14\n",
            "numerical: -235.986217 analytic: -235.986217, relative error: 1.921709e-12\n",
            "numerical: -184.159159 analytic: -184.159159, relative error: 3.277710e-12\n",
            "numerical: -235.986217 analytic: -235.986217, relative error: 1.921709e-12\n",
            "numerical: -1.962288 analytic: -1.962288, relative error: 7.742977e-10\n",
            "numerical: -1.962288 analytic: -1.962288, relative error: 7.742977e-10\n",
            "numerical: -145.674273 analytic: -145.674273, relative error: 1.170627e-14\n",
            "numerical: -235.986217 analytic: -235.986217, relative error: 1.921709e-12\n",
            "Gradient check bias large epsilon\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612564e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612564e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612564e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612564e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612564e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612564e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612564e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612564e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612564e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612564e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612564e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612564e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612564e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612564e-11\n",
            "numerical: -22.466285 analytic: -22.466285, relative error: 7.612564e-11\n"
          ]
        }
      ],
      "source": [
        "loss, dw1, db1 = huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)\n",
        "\n",
        "print(\"Gradient check w\")\n",
        "# Check with numerical gradient w\n",
        "f = lambda w1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)[0]\n",
        "grad_numerical = grad_check_sparse(f, w1, dw1, 15, error=1e-9)\n",
        "\n",
        "print(\"Gradient check bias\")\n",
        "# Check with numerical gradient b\n",
        "f2 = lambda b1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)[0]\n",
        "grad_numerical = grad_check_sparse(f2, b1, db1, 15, error=1e-9)\n",
        "\n",
        "\n",
        "# Large epsilon\n",
        "large_eps_loss, large_eps_dw1, large_eps_db1 = huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)\n",
        "\n",
        "print(\"Gradient check w large epsilon\")\n",
        "# Check with numerical gradient w\n",
        "f = lambda w1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)[0]\n",
        "grad_numerical = grad_check_sparse(f, w1, large_eps_dw1, 15, error=1e-9)\n",
        "\n",
        "print(\"Gradient check bias large epsilon\")\n",
        "# Check with numerical gradient b\n",
        "f2 = lambda b1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)[0]\n",
        "grad_numerical = grad_check_sparse(f2, b1, large_eps_db1, 15, error=1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d367b1c",
      "metadata": {
        "id": "5d367b1c"
      },
      "source": [
        "## with regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "ee9c47a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "ee9c47a5",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "353c4a3ff31d5ae93f310db8aada6b16",
          "grade": true,
          "grade_id": "cell-fc0f22937553dd38",
          "locked": true,
          "points": 1.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "b7d7c45b-3148-4383-c9ae-afaa3c352462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient check w\n",
            "numerical: -219.814163 analytic: -219.814163, relative error: 3.908056e-12\n",
            "numerical: -8290.524118 analytic: -8290.524118, relative error: 3.082652e-13\n",
            "numerical: -50.622006 analytic: -50.622006, relative error: 2.969921e-11\n",
            "numerical: -50.622006 analytic: -50.622006, relative error: 2.969921e-11\n",
            "numerical: -219.814163 analytic: -219.814163, relative error: 3.908056e-12\n",
            "numerical: -145.674197 analytic: -145.674197, relative error: 3.332485e-12\n",
            "numerical: -12.008925 analytic: -12.008925, relative error: 1.058132e-10\n",
            "numerical: -1.962089 analytic: -1.962089, relative error: 6.157627e-10\n",
            "numerical: -50.622006 analytic: -50.622006, relative error: 2.969921e-11\n",
            "numerical: -12.008925 analytic: -12.008925, relative error: 1.058132e-10\n",
            "numerical: -8290.524118 analytic: -8290.524118, relative error: 3.082652e-13\n",
            "numerical: -12.008925 analytic: -12.008925, relative error: 1.058132e-10\n",
            "numerical: -8449.099409 analytic: -8449.099409, relative error: 1.083975e-13\n",
            "numerical: -145.674197 analytic: -145.674197, relative error: 3.332485e-12\n",
            "numerical: -8290.524118 analytic: -8290.524118, relative error: 3.082652e-13\n",
            "Gradient check bias\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111229e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111229e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111229e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111229e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111229e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111229e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111229e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111229e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111229e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111229e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111229e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111229e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111229e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111229e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111229e-11\n",
            "Gradient check w large epsilon\n",
            "numerical: -50.622006 analytic: -50.622006, relative error: 2.969886e-11\n",
            "numerical: -404.549988 analytic: -404.549988, relative error: 1.376930e-12\n",
            "numerical: -235.986078 analytic: -235.986078, relative error: 2.873472e-12\n",
            "numerical: -8449.099409 analytic: -8449.099409, relative error: 1.077516e-13\n",
            "numerical: -1.962089 analytic: -1.962089, relative error: 6.157627e-10\n",
            "numerical: -1.962089 analytic: -1.962089, relative error: 6.157627e-10\n",
            "numerical: -235.986078 analytic: -235.986078, relative error: 2.873472e-12\n",
            "numerical: -404.549988 analytic: -404.549988, relative error: 1.376930e-12\n",
            "numerical: -235.986078 analytic: -235.986078, relative error: 2.873472e-12\n",
            "numerical: -219.814163 analytic: -219.814163, relative error: 3.908056e-12\n",
            "numerical: -184.159425 analytic: -184.159425, relative error: 5.255008e-13\n",
            "numerical: -1.962089 analytic: -1.962089, relative error: 6.157627e-10\n",
            "numerical: -145.674197 analytic: -145.674197, relative error: 3.332972e-12\n",
            "numerical: -332.348044 analytic: -332.348044, relative error: 2.960887e-12\n",
            "numerical: -50.622006 analytic: -50.622006, relative error: 2.969886e-11\n",
            "Gradient check bias large epsilon\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111206e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111206e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111206e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111206e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111206e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111206e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111206e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111206e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111206e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111206e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111206e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111206e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111206e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111206e-11\n",
            "numerical: -22.466392 analytic: -22.466392, relative error: 8.111206e-11\n"
          ]
        }
      ],
      "source": [
        "loss, dw1, db1 = huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)\n",
        "\n",
        "print(\"Gradient check w\")\n",
        "# Check with numerical gradient w\n",
        "f = lambda w1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)[0]\n",
        "grad_numerical = grad_check_sparse(f, w1, dw1, 15, error=1e-9)\n",
        "\n",
        "print(\"Gradient check bias\")\n",
        "# Check with numerical gradient b\n",
        "f2 = lambda b1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)[0]\n",
        "grad_numerical = grad_check_sparse(f2, b1, db1, 15, error=1e-9)\n",
        "\n",
        "\n",
        "# Large epsilon\n",
        "large_eps_loss, large_eps_dw1, large_eps_db1 = huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)\n",
        "\n",
        "print(\"Gradient check w large epsilon\")\n",
        "# Check with numerical gradient w\n",
        "f = lambda w1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)[0]\n",
        "grad_numerical = grad_check_sparse(f, w1, large_eps_dw1, 15, error=1e-9)\n",
        "\n",
        "print(\"Gradient check bias large epsilon\")\n",
        "# Check with numerical gradient b\n",
        "f2 = lambda b1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)[0]\n",
        "grad_numerical = grad_check_sparse(f2, b1, large_eps_db1, 15, error=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "ed260f95",
      "metadata": {
        "deletable": false,
        "id": "ed260f95",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "51a2209d0893d2b94c64bc684159f81c",
          "grade": false,
          "grade_id": "cell-9bc0bf420f2797ba",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class LinearModel():\n",
        "    def __init__(self):\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "\n",
        "    def train(self, X, y, learning_rate=1e-3, alpha=0.0001, num_iters=100, batch_size=200, verbose=False):\n",
        "        N, d = X.shape\n",
        "\n",
        "        if self.w is None: # Initialization\n",
        "            self.w = 0.001 * np.random.randn(d)\n",
        "            self.b = 0.0\n",
        "\n",
        "        # Run stochastic gradient descent to optimize w\n",
        "\n",
        "        loss_history = []\n",
        "        for it in range(num_iters):\n",
        "            X_batch = None\n",
        "            y_batch = None\n",
        "\n",
        "            # Sample batch_size elements in X_batch and y_batch\n",
        "            # X_batch shape is  (batch_size, d) and y_batch shape is (batch_size,)\n",
        "            # Hint: Use np.random.choice to generate indices\n",
        "            # YOUR CODE HERE\n",
        "             # YOUR CODE HERE\n",
        "            index = np.random.choice(N,batch_size,replace=False)\n",
        "            X_batch = np.array([X[i] for i in index])\n",
        "            y_batch = np.array([y[i] for i in index])\n",
        "\n",
        "\n",
        "\n",
        "            # evaluate loss and gradient\n",
        "            loss, dw, db = self.loss(X_batch, y_batch, alpha)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            # perform parameter update\n",
        "            # Update the weights w and bias b using the gradient and the learning rate.\n",
        "            # YOUR CODE HERE\n",
        "            self.w= self.w- learning_rate * dw\n",
        "            self.b = self.b- learning_rate * db\n",
        "\n",
        "\n",
        "\n",
        "            if verbose and it % 10000 == 0:\n",
        "                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
        "\n",
        "        return loss_history\n",
        "\n",
        "    def predict(self, X):\n",
        "        pass\n",
        "\n",
        "    def loss(self, X_batch, y_batch, reg):\n",
        "        pass\n",
        "\n",
        "class HuberRegression(LinearModel):\n",
        "    \"\"\" Linear regression \"\"\"\n",
        "\n",
        "    def loss(self, X_batch, y_batch, alpha):\n",
        "        return huber_loss_vectorized(self.w, self.b, X_batch, y_batch, alpha=alpha)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # YOUR CODE HERE\n",
        "        y = X.dot(self.w) + self.b\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "766c7c05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "766c7c05",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f0baa5b2cf387331bd0a892f09b68ffc",
          "grade": true,
          "grade_id": "cell-43a5575eb3552466",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "981c0752-0dc6-4b47-be50-2b602c74f1a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration 0 / 75000: loss 284.836255\n",
            "iteration 10000 / 75000: loss 7.256567\n",
            "iteration 20000 / 75000: loss 7.165263\n",
            "iteration 30000 / 75000: loss 10.268618\n",
            "iteration 40000 / 75000: loss 11.793308\n",
            "iteration 50000 / 75000: loss 8.294548\n",
            "iteration 60000 / 75000: loss 8.214502\n",
            "iteration 70000 / 75000: loss 13.435476\n",
            "MSE scikit-learn: 24.04102301055778\n",
            "MSE gradient descent model : 21.918054529522752\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train1 = scaler.fit_transform(X_train1)\n",
        "\n",
        "sk_model = HuberRegressor(fit_intercept=True)\n",
        "sk_model.fit(X_train1, y_train1)\n",
        "sk_pred = sk_model.predict(X_train1)\n",
        "sk_mse = mean_squared_error(sk_pred, y_train1)\n",
        "\n",
        "model = HuberRegression()\n",
        "model.train(X_train1, y_train1, num_iters=75000, batch_size=64, learning_rate=1e-2, verbose=True)\n",
        "pred = model.predict(X_train1)\n",
        "mse = mean_squared_error(pred, y_train1)\n",
        "\n",
        "print(\"MSE scikit-learn:\", sk_mse)\n",
        "print(\"MSE gradient descent model :\", mse)\n",
        "assert mse - sk_mse < 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4f24bf4",
      "metadata": {
        "id": "d4f24bf4"
      },
      "source": [
        "# Multinomial logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "b194affb",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b194affb",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fcf6c9bab4975732f03ce6c8215e72be",
          "grade": false,
          "grade_id": "cell-27c3352e3785ecdb",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "data = load_iris()\n",
        "X_train2, y_train2 = data.data, data.target\n",
        "\n",
        "W = np.random.randn(X_train2.shape[1], 3) * 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "d8405c63",
      "metadata": {
        "deletable": false,
        "id": "d8405c63",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "af6fecc0ab19bd2c392db4255a946d03",
          "grade": false,
          "grade_id": "cell-9049b2a8d3edaeaf",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def softmax_loss_naive(W, X, y, alpha):\n",
        "    \"\"\"\n",
        "    Softmax loss function WITH FOR LOOPS\n",
        "\n",
        "    Inputs:\n",
        "    - W: array of shape (D, C) containing weights\n",
        "    - X: array of shape (N, D) containing a minibatch of data\n",
        "    - y: array of shape (N,) containing training labels\n",
        "    - alpha: (float) regularization\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss as single float\n",
        "    - gradient with respect to weights W;  same shape as W\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialization\n",
        "    loss = 0.0\n",
        "    dW = np.zeros_like(W)\n",
        "\n",
        "    # Tandremo ny numeric instability\n",
        "    # YOUR CODE HERE\n",
        "    W=W.T\n",
        "    X=X.T\n",
        "    dW = dW.T\n",
        "    num_classes = W.shape[0]\n",
        "    num_train = X.shape[1]\n",
        "\n",
        "    for i in range(num_train):\n",
        "        # Compute vector of scores\n",
        "        f_i = W.dot(X[:, i]) # in R^{num_classes}\n",
        "\n",
        "        # Normalization trick to avoid numerical instability, per http://cs231n.github.io/linear-classify/#softmax\n",
        "        log_c = np.max(f_i)\n",
        "        f_i -= log_c\n",
        "\n",
        "        # Compute loss (and add to it, divided later)\n",
        "        # L_i = - f(x_i)_{y_i} + log \\sum_j e^{f(x_i)_j}\n",
        "        sum_i = 0.0\n",
        "        for f_i_j in f_i:\n",
        "            sum_i += np.exp(f_i_j)\n",
        "        loss += -f_i[y[i]] + np.log(sum_i)\n",
        "\n",
        "        # Compute gradient\n",
        "        # dw_j = 1/num_train * \\sum_i[x_i * (p(y_i = j)-Ind{y_i = j} )]\n",
        "        # Here we are computing the contribution to the inner sum for a given i.\n",
        "        for j in range(num_classes):\n",
        "            p = np.exp(f_i[j])/sum_i\n",
        "            dW[j, :] += (p-(j == y[i])) * X[:, i]\n",
        "\n",
        "    # Compute average\n",
        "    loss /= num_train\n",
        "    dW /= num_train\n",
        "\n",
        "    # Regularization\n",
        "    loss += 0.5 * alpha * np.sum(W * W)\n",
        "    dW += alpha*W\n",
        "\n",
        "    dW=dW.T\n",
        "\n",
        "    return loss, dW\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b9aa529",
      "metadata": {
        "id": "8b9aa529"
      },
      "source": [
        "## Without regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "0eea225f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "0eea225f",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ac16981288c3c2f604ad8bab7defad33",
          "grade": true,
          "grade_id": "cell-878cebbdfa4e3f9b",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "78602545-b503-4485-f5dc-6521e2c93acd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numerical: -0.275657 analytic: -0.275657, relative error: 6.064547e-11\n",
            "numerical: 0.318071 analytic: 0.318071, relative error: 4.178127e-11\n",
            "numerical: -0.031708 analytic: -0.031708, relative error: 3.978940e-09\n",
            "numerical: -0.598316 analytic: -0.598316, relative error: 1.108522e-10\n",
            "numerical: 0.318071 analytic: 0.318071, relative error: 4.178127e-11\n",
            "numerical: 0.280437 analytic: 0.280437, relative error: 5.015025e-10\n",
            "numerical: 0.027505 analytic: 0.027505, relative error: 8.944895e-10\n",
            "numerical: -0.031708 analytic: -0.031708, relative error: 3.978940e-09\n",
            "numerical: -0.598316 analytic: -0.598316, relative error: 1.108522e-10\n",
            "numerical: 0.318071 analytic: 0.318071, relative error: 4.178127e-11\n",
            "numerical: -0.598316 analytic: -0.598316, relative error: 1.108522e-10\n",
            "numerical: -0.031708 analytic: -0.031708, relative error: 3.978940e-09\n"
          ]
        }
      ],
      "source": [
        "loss, dW = softmax_loss_naive(W, X_train2, y_train2, 0.0)\n",
        "\n",
        "f = lambda W: softmax_loss_naive(W, X_train2, y_train2, 0.0)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, dW, error=1e-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7edea2f2",
      "metadata": {
        "id": "7edea2f2"
      },
      "source": [
        "## With regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "533001ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "533001ff",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "bd9a1e462d23666f55369d6b9d152117",
          "grade": true,
          "grade_id": "cell-ead29b6569263a0f",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "3cc88bf8-8502-4920-91a9-2851b26939a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numerical: -0.598189 analytic: -0.598189, relative error: 1.130301e-10\n",
            "numerical: 0.280336 analytic: 0.280336, relative error: 4.839148e-10\n",
            "numerical: -0.168128 analytic: -0.168128, relative error: 3.210746e-10\n",
            "numerical: -0.248875 analytic: -0.248875, relative error: 6.231874e-10\n",
            "numerical: 0.095317 analytic: 0.095317, relative error: 1.414266e-10\n",
            "numerical: 0.280336 analytic: 0.280336, relative error: 4.839148e-10\n",
            "numerical: 0.095317 analytic: 0.095317, relative error: 1.414266e-10\n",
            "numerical: -0.275813 analytic: -0.275813, relative error: 6.996702e-11\n",
            "numerical: -0.122599 analytic: -0.122599, relative error: 1.279049e-10\n",
            "numerical: -0.031794 analytic: -0.031794, relative error: 3.870524e-09\n",
            "numerical: -0.275813 analytic: -0.275813, relative error: 6.996702e-11\n",
            "numerical: 0.095317 analytic: 0.095317, relative error: 1.414266e-10\n"
          ]
        }
      ],
      "source": [
        "loss, dW = softmax_loss_naive(W, X_train2, y_train2, 2)\n",
        "\n",
        "f = lambda W: softmax_loss_naive(W, X_train2, y_train2, 2)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, dW, error=1e-7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "c2593d82",
      "metadata": {
        "deletable": false,
        "id": "c2593d82",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "48f52a4942e2b468f5775c5c7fbd2617",
          "grade": false,
          "grade_id": "cell-11e6980597b20334",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def softmax_loss_vectorized(W, X, y, alpha, fit_intercept=False):\n",
        "    \"\"\"\n",
        "    Softmax loss function WITHOUT FOR LOOPS\n",
        "\n",
        "    Inputs:\n",
        "    - W: array of shape (D, C) containing weights\n",
        "    - X: array of shape (N, D) containing a minibatch of data\n",
        "    - y: array of shape (N,) containing training labels\n",
        "    - alpha: (float) regularization\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss as single float\n",
        "    - gradient with respect to weights W;  same shape as W\n",
        "    \"\"\"\n",
        "    # Initialize the loss and gradient to zero.\n",
        "    loss = 0.0\n",
        "    dW = np.zeros_like(W)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    num_train = X.shape[0]\n",
        "    f = X.dot(W)\n",
        "    f = f - np.max(f, axis=1)[:, np.newaxis]\n",
        "    loss = -np.sum(np.log(np.exp(f[np.arange(num_train), y]) / np.sum(np.exp(f), axis=1)))\n",
        "    loss /= num_train\n",
        "    loss += 0.5 * alpha * np.sum(W * W)\n",
        "    ind = np.zeros_like(f)\n",
        "    ind[np.arange(num_train), y] = 1\n",
        "    dW = X.T.dot(np.exp(f) / np.sum(np.exp(f), axis=1, keepdims=True) - ind)\n",
        "    dW /= num_train\n",
        "    dW += alpha * W\n",
        "\n",
        "\n",
        "    return loss, dW"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddea508b",
      "metadata": {
        "id": "ddea508b"
      },
      "source": [
        "## Without regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "78f759e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "78f759e2",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "441d40ba642d3a505582eaeb87981127",
          "grade": true,
          "grade_id": "cell-0bc0fcf0bd3d3f13",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "34649b4e-ccd0-4ef6-a877-e95443b05ee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numerical: -0.122878 analytic: -0.122878, relative error: 1.669786e-10\n",
            "numerical: -0.275657 analytic: -0.275657, relative error: 1.990557e-11\n",
            "numerical: -0.598316 analytic: -0.598316, relative error: 8.301847e-11\n",
            "numerical: -0.275657 analytic: -0.275657, relative error: 1.990557e-11\n",
            "numerical: -0.248730 analytic: -0.248730, relative error: 5.342213e-10\n",
            "numerical: 0.280437 analytic: 0.280437, relative error: 4.619135e-10\n",
            "numerical: -0.042414 analytic: -0.042414, relative error: 5.005900e-11\n",
            "numerical: -0.042414 analytic: -0.042414, relative error: 5.005900e-11\n",
            "numerical: 0.318071 analytic: 0.318071, relative error: 1.057590e-11\n",
            "numerical: 0.318071 analytic: 0.318071, relative error: 1.057590e-11\n",
            "numerical: -0.248730 analytic: -0.248730, relative error: 5.342213e-10\n",
            "numerical: 0.280437 analytic: 0.280437, relative error: 4.619135e-10\n"
          ]
        }
      ],
      "source": [
        "loss, dW = softmax_loss_vectorized(W, X_train2, y_train2, 0.0)\n",
        "\n",
        "f = lambda W: softmax_loss_vectorized(W, X_train2, y_train2, 0.0)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, dW, error=1e-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27c84f53",
      "metadata": {
        "id": "27c84f53"
      },
      "source": [
        "## With regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "8bddf43a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "8bddf43a",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6475ed3a14441d5488ecb46ec2522bbb",
          "grade": true,
          "grade_id": "cell-1fe0149d053dade0",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "e3177fa9-c7a6-4755-b481-75060f1ec842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numerical: 0.766373 analytic: 0.766373, relative error: 7.360716e-11\n",
            "numerical: 0.027384 analytic: 0.027384, relative error: 3.779838e-10\n",
            "numerical: -0.031794 analytic: -0.031794, relative error: 4.045124e-09\n",
            "numerical: -0.122599 analytic: -0.122599, relative error: 1.731833e-10\n",
            "numerical: -0.122599 analytic: -0.122599, relative error: 1.731833e-10\n",
            "numerical: -0.598189 analytic: -0.598189, relative error: 8.519046e-11\n",
            "numerical: 0.766373 analytic: 0.766373, relative error: 7.360716e-11\n",
            "numerical: 0.095317 analytic: 0.095317, relative error: 2.080031e-10\n",
            "numerical: -0.275813 analytic: -0.275813, relative error: 1.053847e-11\n",
            "numerical: -0.122599 analytic: -0.122599, relative error: 1.731833e-10\n",
            "numerical: 0.318181 analytic: 0.318181, relative error: 6.465905e-12\n",
            "numerical: 0.318181 analytic: 0.318181, relative error: 6.465905e-12\n"
          ]
        }
      ],
      "source": [
        "loss, dW = softmax_loss_vectorized(W, X_train2, y_train2, 2)\n",
        "\n",
        "f = lambda W: softmax_loss_vectorized(W, X_train2, y_train2, 2)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, dW, error=1e-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab7e247a",
      "metadata": {
        "id": "ab7e247a"
      },
      "source": [
        "## Gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "9dff0bbc",
      "metadata": {
        "deletable": false,
        "id": "9dff0bbc",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a0cedf5c70f5cf04cdd8b74702815dba",
          "grade": false,
          "grade_id": "cell-0f69bb891603b665",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class LinearModel():\n",
        "    def __init__(self, fit_intercept=True):\n",
        "        self.W = None\n",
        "        self.fit_intercept = fit_intercept\n",
        "\n",
        "    def train(self, X, y, learning_rate=1e-3, alpha=0, num_iters=100, batch_size=200, verbose=False):\n",
        "        if self.fit_intercept:\n",
        "            # YOUR CODE HERE\n",
        "            tmp = np.ones((len(X),1))\n",
        "            X = np.append(tmp, X, axis = 1)\n",
        "\n",
        "        N, d = X.shape\n",
        "\n",
        "        C = (np.max(y) + 1)\n",
        "        if self.W is None: # Initialization\n",
        "            self.W = 0.001 * np.random.randn(d, C)\n",
        "\n",
        "        C = (np.max(y) + 1)\n",
        "        if self.W is None: # Initialization\n",
        "            self.W = 0.001 * np.random.randn(d, C)\n",
        "\n",
        "        # Run stochastic gradient descent to optimize W\n",
        "\n",
        "        loss_history = []\n",
        "        for it in range(num_iters):\n",
        "            X_batch = None\n",
        "            y_batch = None\n",
        "\n",
        "            # Sample batch_size elements in X_batch and y_batch\n",
        "            # X_batch shape is  (batch_size, d) and y_batch shape is (batch_size,)\n",
        "            # Hint: Use np.random.choice to generate indices\n",
        "            # YOUR CODE HERE\n",
        "            index = np.random.choice(N,batch_size,replace=False)\n",
        "            X_batch = np.array([X[i] for i in index])\n",
        "            y_batch = np.array([y[i] for i in index])\n",
        "\n",
        "\n",
        "\n",
        "            # evaluate loss and gradient\n",
        "            loss, dW = self.loss(X_batch, y_batch, alpha)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            # perform parameter update\n",
        "            # Update the weights w using the gradient and the learning rate.\n",
        "            # YOUR CODE HERE\n",
        "            self.W= self.W- learning_rate * dW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            if verbose and it % 10000 == 0:\n",
        "                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
        "\n",
        "        return loss_history\n",
        "\n",
        "    def predict(self, X):\n",
        "        pass\n",
        "\n",
        "    def loss(self, X_batch, y_batch, reg):\n",
        "        pass\n",
        "\n",
        "class MultinomialLogisticRegressor(LinearModel):\n",
        "    \"\"\" Softmax regression \"\"\"\n",
        "\n",
        "    def loss(self, X_batch, y_batch, alpha):\n",
        "        return softmax_loss_vectorized(self.W, X_batch, y_batch, alpha)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - X: array of shape (N, D)\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: 1-dimensional array of length N, each element is an integer giving the predicted class\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        if self.fit_intercept:\n",
        "            # YOUR CODE HERE\n",
        "            tmp = np.ones((len(X),1))\n",
        "            X = np.append(tmp, X, axis=1)\n",
        "\n",
        "        z = X.dot(self.W)\n",
        "        y_pred = np.argmax(z, axis=1)\n",
        "\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "ce21aaa4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "ce21aaa4",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "56aacc6a9e4ede50cd934a81dfd9915f",
          "grade": true,
          "grade_id": "cell-8569aecb5759a819",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "f62e4a8b-549f-40fb-97e7-d71765ade698"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration 0 / 75000: loss 1.098483\n",
            "iteration 10000 / 75000: loss 0.402564\n",
            "iteration 20000 / 75000: loss 0.423032\n",
            "iteration 30000 / 75000: loss 0.334499\n",
            "iteration 40000 / 75000: loss 0.393382\n",
            "iteration 50000 / 75000: loss 0.295145\n",
            "iteration 60000 / 75000: loss 0.337588\n",
            "iteration 70000 / 75000: loss 0.356096\n",
            "Accuracy scikit-learn: 0.86\n",
            "Accuracy gradient descent model : 0.8666666666666667\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train2 = scaler.fit_transform(X_train2)\n",
        "\n",
        "sk_model = LogisticRegression(fit_intercept=False)\n",
        "sk_model.fit(X_train2, y_train2)\n",
        "sk_pred = sk_model.predict(X_train2)\n",
        "sk_accuracy = accuracy_score(y_train2, sk_pred)\n",
        "\n",
        "model = MultinomialLogisticRegressor(fit_intercept=False)\n",
        "model.train(X_train2, y_train2, num_iters=75000, batch_size=64, learning_rate=1e-3, verbose=True)\n",
        "pred = model.predict(X_train2)\n",
        "model_accuracy = accuracy_score(y_train2, pred)\n",
        "\n",
        "print(\"Accuracy scikit-learn:\", sk_accuracy)\n",
        "print(\"Accuracy gradient descent model :\", model_accuracy)\n",
        "assert sk_accuracy - model_accuracy < 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "b2c6a8ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "b2c6a8ad",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "40a4732103eaf77860c941e0897de01c",
          "grade": true,
          "grade_id": "cell-30e12569fdfb269c",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "e0dde7fb-4d95-4288-8fd4-85d9b3289a86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration 0 / 75000: loss 1.099141\n",
            "iteration 10000 / 75000: loss 0.337020\n",
            "iteration 20000 / 75000: loss 0.258650\n",
            "iteration 30000 / 75000: loss 0.221944\n",
            "iteration 40000 / 75000: loss 0.207700\n",
            "iteration 50000 / 75000: loss 0.201054\n",
            "iteration 60000 / 75000: loss 0.177119\n",
            "iteration 70000 / 75000: loss 0.139653\n",
            "Accuracy scikit-learn: 0.9733333333333334\n",
            "Accuracy gradient descent model : 0.96\n"
          ]
        }
      ],
      "source": [
        "sk_model = LogisticRegression(fit_intercept=True)\n",
        "sk_model.fit(X_train2, y_train2)\n",
        "sk_pred = sk_model.predict(X_train2)\n",
        "sk_accuracy = accuracy_score(y_train2, sk_pred)\n",
        "\n",
        "model = MultinomialLogisticRegressor(fit_intercept=True)\n",
        "model.train(X_train2, y_train2, num_iters=75000, batch_size=64, learning_rate=1e-3, verbose=True)\n",
        "pred = model.predict(X_train2)\n",
        "model_accuracy = accuracy_score(y_train2, pred)\n",
        "\n",
        "print(\"Accuracy scikit-learn:\", sk_accuracy)\n",
        "print(\"Accuracy gradient descent model :\", model_accuracy)\n",
        "assert sk_accuracy - model_accuracy < 0.02"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62032646",
      "metadata": {
        "id": "62032646"
      },
      "source": [
        "# K-Nearest Neighbor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3c81034",
      "metadata": {
        "id": "a3c81034"
      },
      "source": [
        "## Computing distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "8a97c549",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8a97c549",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "858935a2455a3ca0e416b3084b730e7f",
          "grade": false,
          "grade_id": "cell-9738d380318b956f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "data = load_digits()\n",
        "X_train3, y_train3 = data.data, data.target\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X_train3, y_train3, test_size=0.33, random_state=2)\n",
        "\n",
        "def get_distances_two_loops_with_norm(X_train, X_test):\n",
        "    num_test = X_test.shape[0]\n",
        "    num_train = X_train.shape[0]\n",
        "    distances = np.zeros((num_test, num_train))\n",
        "    for i in range(num_test):\n",
        "        for j in range(num_train):\n",
        "            distances[i, j] = np.linalg.norm(X_test[i] - X_train[j])\n",
        "    return distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "d1b01336",
      "metadata": {
        "deletable": false,
        "id": "d1b01336",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6cfd476a7819d38b3c8ef47443365f64",
          "grade": false,
          "grade_id": "cell-4756294c792f6985",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def get_distances_two_loops(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Compute the distance between each test point in X_test and each training point\n",
        "    in X_train\n",
        "\n",
        "    Inputs:\n",
        "    - X_test: array of shape (num_test, D)\n",
        "\n",
        "    Returns:\n",
        "    - distances: array of shape (num_test, num_train), dists[i, j] is Euclidean distance between\n",
        "    the ith test point and the jth training point.\n",
        "    \"\"\"\n",
        "    num_test = X_test.shape[0]\n",
        "    num_train = X_train.shape[0]\n",
        "    distances = np.zeros((num_test, num_train))\n",
        "    for i in range(num_test):\n",
        "        for j in range(num_train):\n",
        "            # Ataovy ao anaty distances[i, j] ny distance entre ith test point sy th training point\n",
        "            # Aza manao boucle instony ato anatiny\n",
        "            # TSY MAHAZO MAMPIASA np.linalg.norm() :D\n",
        "            # YOUR CODE HERE\n",
        "            distances[i][j] = np.sqrt(np.sum((X_test[i] - X_train[j])**2))\n",
        "\n",
        "    return distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "da674674",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "da674674",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "78d07191456013c3a22619c2d26e2503",
          "grade": true,
          "grade_id": "cell-76b63cae5d6a5977",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "9c196976-724d-4848-fc72-398362945380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0\n"
          ]
        }
      ],
      "source": [
        "distances = get_distances_two_loops(X_train3, X_test3)\n",
        "true_distances = get_distances_two_loops_with_norm(X_train3, X_test3)\n",
        "\n",
        "difference = np.linalg.norm(distances - true_distances, ord='fro')\n",
        "\n",
        "print(difference)\n",
        "assert difference < 1e-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "69a0a04b",
      "metadata": {
        "deletable": false,
        "id": "69a0a04b",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3b98a31f8ab9e6b8bd5a18ea02dd7165",
          "grade": false,
          "grade_id": "cell-05658431df004915",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def compute_distances_one_loop(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Compute the distance between each test point in X_test and each training point\n",
        "    in X_train\n",
        "\n",
        "    Inputs:\n",
        "    - X_test: array of shape (num_test, D)\n",
        "\n",
        "    Returns:\n",
        "    - dists: array of shape (num_test, num_train), dists[i, j] is Euclidean distance between\n",
        "    the ith test point and the jth training point.\n",
        "    \"\"\"\n",
        "    num_test = X_test.shape[0]\n",
        "    num_train = X_train.shape[0]\n",
        "    distances = np.zeros((num_test, num_train))\n",
        "    for i in range(num_test):\n",
        "        # Ataovy ao anaty dists[i, j] ny distance entre ith test point sy th training point\n",
        "        # Aza manao boucle instony ato anatiny\n",
        "        # TSY MAHAZO MAMPIASA np.linalg.norm() :D\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "         distances[i] = np.sqrt(np.sum((X_test[i] - X_train)**2, axis=1))\n",
        "\n",
        "    return distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "2540b717",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "2540b717",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6a9d09964868df2c201e48372082c2e5",
          "grade": true,
          "grade_id": "cell-cfa64f63da5511f9",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "1d32180a-3128-43fe-975f-4ab82f8f5ca1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0\n"
          ]
        }
      ],
      "source": [
        "distances = compute_distances_one_loop(X_train3, X_test3)\n",
        "true_distances = get_distances_two_loops_with_norm(X_train3, X_test3)\n",
        "\n",
        "difference = np.linalg.norm(distances - true_distances, ord='fro')\n",
        "\n",
        "print(difference)\n",
        "assert difference < 1e-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "c3788be0",
      "metadata": {
        "deletable": false,
        "id": "c3788be0",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e10c370f1f9c3e653fc817c883b7de0e",
          "grade": false,
          "grade_id": "cell-3cb10928a6af9889",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def get_distances_zero_loop(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Compute the distance between each test point in X_test and each training point\n",
        "    in X_train\n",
        "\n",
        "    Inputs:\n",
        "    - X_test: array of shape (num_test, D)\n",
        "\n",
        "    Returns:\n",
        "    - distances: array of shape (num_test, num_train), dists[i, j] is Euclidean distance between\n",
        "    the ith test point and the jth training point.\n",
        "    \"\"\"\n",
        "    num_test = X_test.shape[0]\n",
        "    num_train = X_train.shape[0]\n",
        "    distances = np.zeros((num_test, num_train))\n",
        "    # Ataovy ao anaty dists[i, j] ny distance entre ith test point sy th training point\n",
        "    # Aza manao boucle instony\n",
        "    # TSY MAHAZO MAMPIASA np.linalg.norm() NA FONCTIONS AO AMIN'NY SCIPY :D\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    M = X_test.dot(X_train.T)\n",
        "    nrow = M.shape[0]\n",
        "    ncol = M.shape[1]\n",
        "\n",
        "    te = np.diag(X_test.dot(X_test.T))\n",
        "    tr = np.diag(X_train.dot(X_train.T))\n",
        "\n",
        "    te = np.reshape(np.repeat(te, ncol), M.shape)\n",
        "    tr = np.reshape(np.repeat(tr, nrow), M.T.shape)\n",
        "\n",
        "    sq = -2 * M + te + tr.T\n",
        "    distances = np.sqrt(sq)\n",
        "\n",
        "    return distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "81a9294b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "81a9294b",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "08a63c2a5bf229c55db5bdc793f5c885",
          "grade": true,
          "grade_id": "cell-ff999d4dddc0760f",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "ff66be79-1d9d-4bbc-c742-b361b9a6c3ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0\n"
          ]
        }
      ],
      "source": [
        "distances = get_distances_zero_loop(X_train3, X_test3)\n",
        "true_distances = get_distances_two_loops_with_norm(X_train3, X_test3)\n",
        "\n",
        "difference = np.linalg.norm(distances - true_distances, ord='fro')\n",
        "\n",
        "print(difference)\n",
        "assert difference < 1e-10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc9c2ac6",
      "metadata": {
        "id": "bc9c2ac6"
      },
      "source": [
        "## K-Nearest Neighbor (knn) classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "2ce1e0f2",
      "metadata": {
        "deletable": false,
        "id": "2ce1e0f2",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c2213945856056edb1cbc5174c98be4e",
          "grade": false,
          "grade_id": "cell-8d852d04f1f1e1c6",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class KNearestNeighborClassifier():\n",
        "    \"\"\" kNN classifier using L2 distance \"\"\"\n",
        "\n",
        "    def __init__(self, k=1):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - k: number of nearest neighbors that vote for the predicted labels.\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the classifier. Just memorize the training data.\n",
        "\n",
        "        Inputs:\n",
        "        - X: array of shape (num_train, D)\n",
        "        - y: array of shape (N,)\n",
        "        \"\"\"\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict labels for test data using this classifier.\n",
        "\n",
        "        Inputs:\n",
        "        - X: array of shape (num_test, D)\n",
        "\n",
        "        Returns:\n",
        "        - y: array of shape (num_test,)\n",
        "        \"\"\"\n",
        "        distances = get_distances_zero_loop(self.X_train, X)\n",
        "        return self.predict_labels(distances)\n",
        "\n",
        "    def predict_labels(self, distances):\n",
        "        \"\"\"\n",
        "        Given a matrix of distances between test points and training points,\n",
        "        predict a label for each test point.\n",
        "\n",
        "        Inputs:\n",
        "        - distances: array of shape (num_test, num_train), dists[i, j] is Euclidean distance between\n",
        "        the ith test point and the jth training point.\n",
        "\n",
        "        Returns:\n",
        "        - y:  array of shape (num_test,)\n",
        "        \"\"\"\n",
        "        num_test = distances.shape[0]\n",
        "        y_pred = np.zeros(num_test)\n",
        "        for i in range(num_test):\n",
        "            # list storing the labels of the k nearest neighbors to the ith test point.\n",
        "            closest_y = []\n",
        "\n",
        "            # Ampidirina ao anaty closest_y ny labels an'ny k neighbors akaiky indrindra\n",
        "            # Jereo fampiasana np.argsort\n",
        "            # YOUR CODE HERE\n",
        "            r = np.argsort(distances[i, :])[:self.k]\n",
        "            closest_y = self.y_train[r]\n",
        "\n",
        "\n",
        "\n",
        "            # Tadiavo ny label betsaka indrindra dia iny no atao prediction\n",
        "            # Raha misy mitovy dia izay label kely raisina\n",
        "            # YOUR CODE HERE\n",
        "            y_pred[i] = np.bincount(closest_y).argmax()\n",
        "\n",
        "\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "c419c1ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "c419c1ac",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e5ccea2db11ef9d1310203f6b2227fbc",
          "grade": true,
          "grade_id": "cell-301db44591c60d17",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "97dd6426-9f5b-4aa9-de9a-3c80cc2379b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy scikit-learn: 0.9831649831649831\n",
            "Accuracy gradient descent model : 0.9831649831649831\n"
          ]
        }
      ],
      "source": [
        "sk_model = KNeighborsClassifier(n_neighbors=3)\n",
        "sk_model.fit(X_train3, y_train3)\n",
        "sk_pred = sk_model.predict(X_test3)\n",
        "sk_accuracy = accuracy_score(y_test3, sk_pred)\n",
        "\n",
        "model = KNearestNeighborClassifier(k=3)\n",
        "model.fit(X_train3, y_train3)\n",
        "pred = model.predict(X_test3)\n",
        "model_accuracy = accuracy_score(y_test3, pred)\n",
        "\n",
        "print(\"Accuracy scikit-learn:\", sk_accuracy)\n",
        "print(\"Accuracy gradient descent model :\", model_accuracy)\n",
        "assert sk_accuracy - model_accuracy < 1e-10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "792bbc93",
      "metadata": {
        "id": "792bbc93"
      },
      "source": [
        "## cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "d4c44832",
      "metadata": {
        "deletable": false,
        "id": "d4c44832",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0433a47c347be926b4625f181c01d120",
          "grade": false,
          "grade_id": "cell-adaaf6c0d8cdce0a",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "num_folds = 5\n",
        "k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]\n",
        "\n",
        "X_train_folds = []\n",
        "y_train_folds = []\n",
        "\n",
        "# Split up the data into folds\n",
        "# X_train_folds and y_train_folds lits of length num_folds\n",
        "\n",
        "# YOUR CODE HERE\n",
        "X_train, y_train = data.data, data.target\n",
        "\n",
        "X_train_folds = np.array_split(X_train, num_folds)\n",
        "y_train_folds = np.array_split(y_train, num_folds)\n",
        "\n",
        "\n",
        "# A dictionary of length num_folds holding the accuracies for different values of k\n",
        "k_to_accuracies = {}\n",
        "\n",
        "# Ataovy ary ilay k-fold cross validation\n",
        "# Atao ao anaty k_to_accuracies ny accuracy isaky ny valeur k\n",
        "# YOUR CODE HERE\n",
        "for k in k_choices:\n",
        "    model_cv = KNearestNeighborClassifier(k)\n",
        "    k_to_accuracies[k] = np.zeros(num_folds)\n",
        "    for i in range(num_folds):\n",
        "        # test fold X_test_vc | y_train_cv\n",
        "        X_test_cv = X_train_folds[i]\n",
        "        y_test_cv = y_train_folds[i]\n",
        "\n",
        "        # training folds X_train_cv | y_train_cv\n",
        "        X_train_cv = None\n",
        "        y_train_cv = None\n",
        "        for j in range(num_folds):\n",
        "            if i != j:\n",
        "                if X_train_cv is None:\n",
        "                    X_train_cv = X_train_folds[j]\n",
        "                    y_train_cv = y_train_folds[j]\n",
        "                else:\n",
        "                    X_train_cv = np.append(X_train_cv, X_train_folds[j], axis=0)\n",
        "                    y_train_cv = np.append(y_train_cv, y_train_folds[j], axis=0)\n",
        "\n",
        "        # cross validation\n",
        "        model_cv.fit(X_train_cv, y_train_cv)\n",
        "        pred_cv = model_cv.predict(X_test_cv)\n",
        "\n",
        "        # computing accuracy\n",
        "        accuracy = accuracy_score(y_test_cv, pred_cv)\n",
        "        k_to_accuracies[k][i] = accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "110ef5f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "110ef5f7",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1f539a48494772e41230a2b590897508",
          "grade": true,
          "grade_id": "cell-4d0c3c0a08ed3f82",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "7cf53421-bdcf-496d-b6af-138933afbfbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k = 1, accuracy = 0.961111\n",
            "k = 1, accuracy = 0.952778\n",
            "k = 1, accuracy = 0.966574\n",
            "k = 1, accuracy = 0.988858\n",
            "k = 1, accuracy = 0.955432\n",
            "k = 3, accuracy = 0.955556\n",
            "k = 3, accuracy = 0.961111\n",
            "k = 3, accuracy = 0.963788\n",
            "k = 3, accuracy = 0.986072\n",
            "k = 3, accuracy = 0.966574\n",
            "k = 5, accuracy = 0.950000\n",
            "k = 5, accuracy = 0.963889\n",
            "k = 5, accuracy = 0.963788\n",
            "k = 5, accuracy = 0.980501\n",
            "k = 5, accuracy = 0.963788\n",
            "k = 8, accuracy = 0.941667\n",
            "k = 8, accuracy = 0.961111\n",
            "k = 8, accuracy = 0.966574\n",
            "k = 8, accuracy = 0.974930\n",
            "k = 8, accuracy = 0.949861\n",
            "k = 10, accuracy = 0.938889\n",
            "k = 10, accuracy = 0.952778\n",
            "k = 10, accuracy = 0.966574\n",
            "k = 10, accuracy = 0.974930\n",
            "k = 10, accuracy = 0.949861\n",
            "k = 12, accuracy = 0.941667\n",
            "k = 12, accuracy = 0.955556\n",
            "k = 12, accuracy = 0.966574\n",
            "k = 12, accuracy = 0.974930\n",
            "k = 12, accuracy = 0.949861\n",
            "k = 15, accuracy = 0.941667\n",
            "k = 15, accuracy = 0.955556\n",
            "k = 15, accuracy = 0.966574\n",
            "k = 15, accuracy = 0.972145\n",
            "k = 15, accuracy = 0.947075\n",
            "k = 20, accuracy = 0.930556\n",
            "k = 20, accuracy = 0.944444\n",
            "k = 20, accuracy = 0.963788\n",
            "k = 20, accuracy = 0.963788\n",
            "k = 20, accuracy = 0.944290\n",
            "k = 50, accuracy = 0.922222\n",
            "k = 50, accuracy = 0.919444\n",
            "k = 50, accuracy = 0.924791\n",
            "k = 50, accuracy = 0.949861\n",
            "k = 50, accuracy = 0.922006\n",
            "k = 100, accuracy = 0.902778\n",
            "k = 100, accuracy = 0.888889\n",
            "k = 100, accuracy = 0.883008\n",
            "k = 100, accuracy = 0.930362\n",
            "k = 100, accuracy = 0.888579\n"
          ]
        }
      ],
      "source": [
        "for k in sorted(k_to_accuracies):\n",
        "    for accuracy in k_to_accuracies[k]:\n",
        "        print('k = %d, accuracy = %f' % (k, accuracy))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
