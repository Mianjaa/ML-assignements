{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1ebb2d94",
      "metadata": {
        "id": "1ebb2d94"
      },
      "source": [
        "# MISA\n",
        "Alohan'ny mamerina dia avereno atao Run ny notebook iray manontolo. Ny fanaovana azy dia redémarrena mihitsy ny kernel aloha (jereo menubar, safidio **Kernel$\\rightarrow$Restart Kernel and Run All Cells**).\n",
        "\n",
        "Izay misy hoe `YOUR CODE HERE` na `YOUR ANSWER HERE` ihany no fenoina. Afaka manampy cells vaovao raha ilaina. Aza adino ny mameno references eo ambany raha ilaina."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80724591",
      "metadata": {
        "id": "80724591"
      },
      "source": [
        "## References\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66f037c4",
      "metadata": {
        "id": "66f037c4"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8c5b7092-6350-41b0-8904-75e1bc12d525",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c5b7092-6350-41b0-8904-75e1bc12d525",
        "outputId": "a3d2115f-623c-4f48-d3b0-1f95d84d3356",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn==1.1.3\n",
            "  Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (3.2.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 0.17.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scikit-learn-1.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn==1.1.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4eec0cbd",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "4eec0cbd",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f7791ca10232655ac53dd4bba44138d2",
          "grade": false,
          "grade_id": "cell-bf951004994ebba2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition  import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_boston, load_iris, load_breast_cancer, make_blobs\n",
        "import numpy as np\n",
        "from random import randrange\n",
        "from sklearn.metrics import accuracy_score\n",
        "import cvxpy as cp\n",
        "\n",
        "def grad_check_sparse(f, x, analytic_grad, num_checks=12, h=1e-5, error=1e-9):\n",
        "    \"\"\"\n",
        "    sample a few random elements and only return numerical\n",
        "    in this dimensions\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(num_checks):\n",
        "        ix = tuple([randrange(m) for m in x.shape])\n",
        "\n",
        "        oldval = x[ix]\n",
        "        x[ix] = oldval + h  # increment by h\n",
        "        fxph = f(x)  # evaluate f(x + h)\n",
        "        x[ix] = oldval - h  # increment by h\n",
        "        fxmh = f(x)  # evaluate f(x - h)\n",
        "        x[ix] = oldval  # reset\n",
        "\n",
        "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
        "        grad_analytic = analytic_grad[ix]\n",
        "        rel_error = abs(grad_numerical - grad_analytic) / (\n",
        "            abs(grad_numerical) + abs(grad_analytic)\n",
        "        )\n",
        "        print(\n",
        "            \"numerical: %f analytic: %f, relative error: %e\"\n",
        "            % (grad_numerical, grad_analytic, rel_error)\n",
        "        )\n",
        "        assert rel_error < error\n",
        "\n",
        "def rel_error(x, y):\n",
        "    \"\"\" returns relative error \"\"\"\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef140bb9",
      "metadata": {
        "id": "ef140bb9"
      },
      "source": [
        "# PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e8e97248",
      "metadata": {
        "deletable": false,
        "id": "e8e97248",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b94485256a5b66d80cbb30860292ac41",
          "grade": false,
          "grade_id": "cell-a61ae6533b88fb95",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class PrincipalComponentAnalysis():\n",
        "    def __init__(self, n_components):\n",
        "        self.n_components = n_components\n",
        "        self.components = None\n",
        "\n",
        "    def fit(self, X):\n",
        "\n",
        "            U,S,V = np.linalg.svd(X)\n",
        "            self.components =V.T[:, :self.n_components]\n",
        "\n",
        "\n",
        "    def transform(self, X):\n",
        "        # YOUR CODE HERE\n",
        "        z = X.dot(self.components)\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c46e59cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "c46e59cf",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "23e107a1a8ddddc51278cf396f27f76f",
          "grade": true,
          "grade_id": "cell-01016aef983d4a70",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "cce90391-95a0-443d-fcd5-b1ab78b542f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.53330964952525e-13\n"
          ]
        }
      ],
      "source": [
        "X_centered = X - np.mean(X, axis=0)\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "pca.fit(X_centered)\n",
        "X_pca_trans = pca.transform(X_centered)\n",
        "\n",
        "model = PrincipalComponentAnalysis(n_components=3)\n",
        "model.fit(X_centered)\n",
        "X_model_trans = model.transform(X_centered)\n",
        "\n",
        "sign_correct_X_model_trans = np.concatenate([np.expand_dims(X_model_trans[:,0],axis=1),-X_model_trans[:,1:]],axis=1)\n",
        "\n",
        "error = rel_error(X_pca_trans, sign_correct_X_model_trans)\n",
        "print(error)\n",
        "assert  error < 1e-11"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aae54f61",
      "metadata": {
        "id": "aae54f61"
      },
      "source": [
        "# Binary Linear SVM with CVXPY"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f038eeec",
      "metadata": {
        "id": "f038eeec"
      },
      "source": [
        "## Hard margin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ae0eae3d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ae0eae3d",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "67a78e61592d16446bae3f77e17ceaea",
          "grade": false,
          "grade_id": "cell-225fb5eb2216ae3b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "X2, y2 = make_blobs(n_samples=300, centers=2, n_features=12, random_state=47)\n",
        "scaler = StandardScaler()\n",
        "X2 = scaler.fit_transform(X2)\n",
        "y2[y2 == 0] = -1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f1bc8a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "00f1bc8a",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "fb10a2e1c603f3c29915c7cac6bf7866",
          "grade": false,
          "grade_id": "cell-b459cc87f3d8726e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "$$\\min_{\\mathbf{w},b}\\frac{1}{2}||\\mathbf{w}||^2$$ <br>\n",
        "$$\\text{s.t } y_i(\\mathbf{w}^{\\top}\\mathbf{x}_i + b) \\ge 1, \\ i=1...N$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "829be3d2",
      "metadata": {
        "deletable": false,
        "id": "829be3d2",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1bcc3a988e383b297c2805d1f6aca7dc",
          "grade": false,
          "grade_id": "cell-d04165c70ffbb870",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class LinearSVMHardMargin():\n",
        "    def __init__(self):\n",
        "        self.w = None\n",
        "        self.b = 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n",
        "        n = X.shape[0]\n",
        "        d = X.shape[1]\n",
        "\n",
        "        w = cp.Variable((d,1))\n",
        "        b = cp.Variable((n,1))\n",
        "\n",
        "        c =cp.norm(w) ** 2\n",
        "        f = (1/2) * (c)\n",
        "        a = cp.Minimize(f)\n",
        "        prob = cp.Problem(a, [y @ (X @ w + b) >= 1])\n",
        "        prob.solve()\n",
        "\n",
        "        self.w = w.value\n",
        "        self.b = b.value\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return the predicted label 1 or -1\"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n",
        "        y_pred = (X.dot(self.w) + self.b)\n",
        "\n",
        "        y_pred[y_pred > 0] = 1\n",
        "        y_pred[y_pred < 0] = -1\n",
        "\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "51c5cf5a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "51c5cf5a",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4fcf9bdd4f28f6b42218539a988b638b",
          "grade": true,
          "grade_id": "cell-5d1f0df83bcb86d1",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "82cb9bd5-2138-4a4e-e14b-f25598545ba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n"
          ]
        }
      ],
      "source": [
        "model = LinearSVMHardMargin()\n",
        "model.fit(X2, y2)\n",
        "pred = model.predict(X2)\n",
        "accuracy = accuracy_score(y2, pred)\n",
        "print(accuracy)\n",
        "assert accuracy == 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03e3371a",
      "metadata": {
        "id": "03e3371a"
      },
      "source": [
        "## Soft Margin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "03e3bfe9",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "03e3bfe9",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "14637e9c774165ae0c123891964870b3",
          "grade": false,
          "grade_id": "cell-e1ffa5b41b5d9d99",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "data3 = load_breast_cancer()\n",
        "X3, y3 = data3.data, data3.target\n",
        "scaler = StandardScaler()\n",
        "X3 = scaler.fit_transform(X3)\n",
        "y3[y3 == 0] = -1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86939d9a",
      "metadata": {
        "id": "86939d9a"
      },
      "source": [
        "$$L(\\mathbf{w},b) = \\frac{1}{N} \\sum_{i=1}^N \\max(0, y_i(\\mathbf{w}^{\\top}\\mathbf{x}_i + b)) + \\lambda||\\mathbf{w}||^2_2$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "118e55f7",
      "metadata": {
        "deletable": false,
        "id": "118e55f7",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6723ed1e25d3de291f7156530cf97613",
          "grade": false,
          "grade_id": "cell-dff3e00a210cd783",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class LinearSVMSoftMargin():\n",
        "    def __init__(self, alpha=0):\n",
        "        self.w = None\n",
        "        self.b = 0\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        #raise NotImplementedError()\n",
        "        n = len(y)\n",
        "        d= X.shape[1]\n",
        "\n",
        "        w = cp.Variable((d,1))\n",
        "        b= cp.Variable((n,1))\n",
        "\n",
        "\n",
        "        c = y @ (X @ w+b)\n",
        "        e = cp.norm2(w)**2\n",
        "        L = (1/n)*cp.sum(cp.pos(1-c))+ self.alpha*(e)\n",
        "\n",
        "        prob = cp.Problem(cp.Minimize(L))\n",
        "        prob.solve()\n",
        "\n",
        "        self.w = w.value\n",
        "        self.b = b.value\n",
        "    def predict(self, X):\n",
        "\n",
        "        \"\"\"Return the predicted label 1 or -1\"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        y_pred = (X.dot(self.w) + self.b)\n",
        "\n",
        "        y_pred[y_pred > 0] = 1\n",
        "        y_pred[y_pred < 0] = -1\n",
        "\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "716b990e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "716b990e",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6c58313a6f325f89e25254d2bc3aecae",
          "grade": true,
          "grade_id": "cell-915364d1b5f54b7e",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "cb2d0157-11d4-4e84-da27-5f0d1443eb1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n"
          ]
        }
      ],
      "source": [
        "model = LinearSVMSoftMargin(alpha=1e-3)\n",
        "model.fit(X3, y3)\n",
        "pred = model.predict(X3)\n",
        "accuracy = accuracy_score(y3, pred)\n",
        "print(accuracy)\n",
        "assert accuracy >= 0.98"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fc3b15c",
      "metadata": {
        "id": "0fc3b15c"
      },
      "source": [
        "# Multiclass Linear SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "549dff61",
      "metadata": {
        "id": "549dff61"
      },
      "source": [
        "## Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0de9d832",
      "metadata": {
        "id": "0de9d832"
      },
      "source": [
        "$$L(\\mathbf{W}) = \\sum_{i=1}^N \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i} + 1) + \\lambda||\\mathbf{w}||^2_2$$ <br>\n",
        "$$\\text{where } s_j = (f(\\mathbf{x}_i;\\mathbf{W}))_j = (\\mathbf{W}\\mathbf{x}_i)_j \\text{ is the score for the j-th class}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "6d4d5e7a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "6d4d5e7a",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "12b5f742a05bdaf1b6adf2bb6083dcd3",
          "grade": false,
          "grade_id": "cell-67c81d664d204471",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "W = np.random.randn(X.shape[1], 3) * 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "fd15eccd",
      "metadata": {
        "deletable": false,
        "id": "fd15eccd",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "53138b3da2e0d48438be8ea853f14eca",
          "grade": false,
          "grade_id": "cell-a015ff237ad977a1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def svm_loss_naive(W, X, y, alpha):\n",
        "    \"\"\"\n",
        "    Multiclass SVM loss function WITH FOR LOOPS\n",
        "\n",
        "    Inputs:\n",
        "    - W: array of shape (D, C) containing weights\n",
        "    - X: array of shape (N, D) containing a minibatch of data\n",
        "    - y: array of shape (N,) containing training labels\n",
        "    - alpha: (float) regularization\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss as single float\n",
        "    - gradient with respect to weights W;  same shape as W\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialization\n",
        "    loss = 0.0\n",
        "    dW = np.zeros_like(W)\n",
        "    num_classes = W.shape[1]\n",
        "    num_train = X.shape[0]\n",
        "    loss = 0.0\n",
        "\n",
        "\n",
        "    for i in range(num_train):\n",
        "        scores = X[i].dot(W)\n",
        "        correct_class_score = scores[y[i]]\n",
        "\n",
        "        for j in range(num_classes):\n",
        "            if j == y[i]:\n",
        "                continue\n",
        "\n",
        "            margin = scores[j] - correct_class_score + 1\n",
        "\n",
        "            if margin>0:\n",
        "                loss += margin\n",
        "                dW[:,y[i]] -= X[i,:]\n",
        "                dW[:,j] += X[i,:]\n",
        "\n",
        "    # Averaging over all examples\n",
        "    loss /= num_train\n",
        "    dW /= num_train\n",
        "\n",
        "    # Add regularization\n",
        "    s = alpha * np.sum(W * W)\n",
        "    loss += 0.5 * s\n",
        "    dW += alpha * W\n",
        "\n",
        "    return loss, dW\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "39b07bb6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "39b07bb6",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "34ca0a0a01822fa2a097921b9df2b6c5",
          "grade": true,
          "grade_id": "cell-94feff1af9a3c069",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "b950e14c-7372-4e60-d958-dfcd85f023ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numerical: -0.370667 analytic: -0.370667, relative error: 2.136525e-11\n",
            "numerical: -0.744667 analytic: -0.744667, relative error: 4.991330e-11\n",
            "numerical: -0.092667 analytic: -0.092667, relative error: 1.433723e-10\n",
            "numerical: -0.502000 analytic: -0.502000, relative error: 2.214475e-12\n",
            "numerical: 0.083333 analytic: 0.083333, relative error: 3.964115e-10\n",
            "numerical: 0.083333 analytic: 0.083333, relative error: 3.964115e-10\n",
            "numerical: -0.744667 analytic: -0.744667, relative error: 4.991330e-11\n",
            "numerical: 0.083333 analytic: 0.083333, relative error: 3.964115e-10\n",
            "numerical: 0.083333 analytic: 0.083333, relative error: 3.964115e-10\n",
            "numerical: -0.092667 analytic: -0.092667, relative error: 1.433723e-10\n",
            "numerical: 0.837333 analytic: 0.837333, relative error: 2.852025e-11\n",
            "numerical: -1.794000 analytic: -1.794000, relative error: 9.448159e-12\n"
          ]
        }
      ],
      "source": [
        "# NO REGLARIZATION\n",
        "loss, dW = svm_loss_naive(W, X, y, 0.0)\n",
        "\n",
        "f = lambda W: svm_loss_naive(W, X, y, 0.0)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, dW, error=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "58d52fb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "58d52fb4",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0df6e08de38631bb7c4d8ce16091899a",
          "grade": true,
          "grade_id": "cell-37dfe1a4bbbdf142",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "7e761aa6-7d3d-45f5-f930-232d115df711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numerical: -0.502054 analytic: -0.502054, relative error: 9.874845e-13\n",
            "numerical: 0.287395 analytic: 0.287395, relative error: 1.534684e-10\n",
            "numerical: -0.744710 analytic: -0.744710, relative error: 4.596918e-11\n",
            "numerical: -1.794161 analytic: -1.794161, relative error: 9.807327e-12\n",
            "numerical: -0.826312 analytic: -0.826312, relative error: 3.023860e-11\n",
            "numerical: -1.794161 analytic: -1.794161, relative error: 9.807327e-12\n",
            "numerical: 0.953044 analytic: 0.953044, relative error: 4.058021e-11\n",
            "numerical: -0.126266 analytic: -0.126266, relative error: 1.634139e-11\n",
            "numerical: -0.370462 analytic: -0.370462, relative error: 2.780455e-11\n",
            "numerical: 0.837592 analytic: 0.837592, relative error: 2.923900e-11\n",
            "numerical: 0.287395 analytic: 0.287395, relative error: 1.534684e-10\n",
            "numerical: -0.092697 analytic: -0.092697, relative error: 1.292553e-10\n"
          ]
        }
      ],
      "source": [
        "#REGLARIZATION\n",
        "loss, dW = svm_loss_naive(W, X, y, 2)\n",
        "\n",
        "f = lambda W: svm_loss_naive(W, X, y, 2)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, dW, error=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "3ecca1d8",
      "metadata": {
        "deletable": false,
        "id": "3ecca1d8",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a5ebb303976d8369a9bb802d3976cad1",
          "grade": false,
          "grade_id": "cell-7e17d2c0c1de2480",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def svm_loss_vectorized(W, X, y, alpha):\n",
        "    \"\"\"\n",
        "    Multiclass SVM loss function WITHOUT FOR LOOPS\n",
        "\n",
        "    Inputs:\n",
        "    - W: array of shape (D, C) containing weights\n",
        "    - X: array of shape (N, D) containing a minibatch of data\n",
        "    - y: array of shape (N,) containing training labels\n",
        "    - alpha: (float) regularization\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss as single float\n",
        "    - gradient with respect to weights W;  same shape as W\n",
        "    \"\"\"\n",
        "    # Initialize the loss and gradient to zero.\n",
        "    loss = 0.0\n",
        "    dW = np.zeros_like(W)\n",
        "    # raise NotImplementedError()\n",
        "\n",
        "    # CODE FROM\n",
        "    # https://mlxai.github.io/2017/01/06/vectorized-implementation-of-svm-loss-and-gradient-update.html\n",
        "\n",
        "    num_train = X.shape[0]\n",
        "\n",
        "    scores = X.dot(W)\n",
        "    yi_scores = scores[np.arange(scores.shape[0]),y] # http://stackoverflow.com/a/23435843/459241\n",
        "    margins = np.maximum(0, scores - np.matrix(yi_scores).T + 1)\n",
        "    margins[np.arange(num_train),y] = 0\n",
        "    loss = np.mean(np.sum(margins, axis=1))\n",
        "    loss += 0.5 * alpha * np.sum(W * W)\n",
        "\n",
        "    binary = margins\n",
        "    binary[margins > 0] = 1\n",
        "    row_sum = np.sum(binary, axis=1)\n",
        "    binary[np.arange(num_train), y] = -row_sum.T\n",
        "    dW = np.dot(X.T, binary)\n",
        "\n",
        "    # Average\n",
        "    dW /= num_train\n",
        "\n",
        "    # Regularize\n",
        "    dW += alpha * W\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "\n",
        "    return loss, dW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "0fa38170",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "0fa38170",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "884d69e79b74e6756f42d7927309de82",
          "grade": true,
          "grade_id": "cell-1bf11ff8259d6847",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "93eb7f7f-8c50-44ed-8f71-f1d36f27e180"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numerical: 2.296000 analytic: 2.296000, relative error: 4.480543e-12\n",
            "numerical: -0.370667 analytic: -0.370667, relative error: 8.587256e-12\n",
            "numerical: -0.092667 analytic: -0.092667, relative error: 9.625027e-11\n",
            "numerical: -0.826667 analytic: -0.826667, relative error: 1.290500e-12\n",
            "numerical: -0.092667 analytic: -0.092667, relative error: 9.625027e-11\n",
            "numerical: -0.826667 analytic: -0.826667, relative error: 1.290500e-12\n",
            "numerical: 0.953333 analytic: 0.953333, relative error: 1.149896e-12\n",
            "numerical: 0.083333 analytic: 0.083333, relative error: 1.299530e-10\n",
            "numerical: 0.953333 analytic: 0.953333, relative error: 1.149896e-12\n",
            "numerical: -0.370667 analytic: -0.370667, relative error: 8.587256e-12\n",
            "numerical: 0.083333 analytic: 0.083333, relative error: 1.299530e-10\n",
            "numerical: -1.794000 analytic: -1.794000, relative error: 1.651101e-13\n"
          ]
        }
      ],
      "source": [
        "# NO REGLARIZATION\n",
        "loss, dW = svm_loss_vectorized(W, X, y, 0.0)\n",
        "\n",
        "f = lambda W: svm_loss_vectorized(W, X, y, 0.0)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, dW, error=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "66b82330",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "66b82330",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "53c5c62906c11829422603614eae7ba7",
          "grade": true,
          "grade_id": "cell-33078afde070ae4f",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "8966916a-6423-4ecc-9c41-4c942ed06237"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numerical: 0.287395 analytic: 0.287395, relative error: 2.037028e-11\n",
            "numerical: 0.837592 analytic: 0.837592, relative error: 3.898145e-12\n",
            "numerical: -0.092697 analytic: -0.092697, relative error: 1.102878e-10\n",
            "numerical: -0.092697 analytic: -0.092697, relative error: 1.102878e-10\n",
            "numerical: -0.502054 analytic: -0.502054, relative error: 9.885902e-13\n",
            "numerical: -0.126266 analytic: -0.126266, relative error: 1.042671e-10\n",
            "numerical: -0.826312 analytic: -0.826312, relative error: 3.350909e-12\n",
            "numerical: -0.744710 analytic: -0.744710, relative error: 6.210871e-12\n",
            "numerical: -0.502054 analytic: -0.502054, relative error: 9.885902e-13\n",
            "numerical: -1.794161 analytic: -1.794161, relative error: 5.251118e-13\n",
            "numerical: -0.744710 analytic: -0.744710, relative error: 6.210871e-12\n",
            "numerical: 0.287395 analytic: 0.287395, relative error: 2.037028e-11\n"
          ]
        }
      ],
      "source": [
        "# REGLARIZATION\n",
        "loss, dW = svm_loss_vectorized(W, X, y, 2)\n",
        "\n",
        "f = lambda W: svm_loss_vectorized(W, X, y, 2)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, dW, error=1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55615d17",
      "metadata": {
        "id": "55615d17"
      },
      "source": [
        "## Gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "366dcd62",
      "metadata": {
        "deletable": false,
        "id": "366dcd62",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2f1639f76b795c999d467767430e2778",
          "grade": false,
          "grade_id": "cell-41975286849a6d83",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class LinearModel():\n",
        "    def __init__(self, fit_intercept=True):\n",
        "        self.W = None\n",
        "        self.fit_intercept = fit_intercept\n",
        "\n",
        "    def train(self, X, y, learning_rate=1e-3, alpha=0, num_iters=100, batch_size=200, verbose=False):\n",
        "        if self.fit_intercept:\n",
        "            # YOUR CODE HERE\n",
        "            tmp = np.ones((X.shape[0], 1))\n",
        "            X = np.concatenate((tmp, X), axis=1)\n",
        "\n",
        "            #raise NotImplementedError()\n",
        "\n",
        "        N, d = X.shape\n",
        "\n",
        "\n",
        "        C = (np.max(y) + 1)\n",
        "        if self.W is None: # Initialization\n",
        "            self.W = 0.001 * np.random.randn(d, C)\n",
        "\n",
        "        # Run stochastic gradient descent to optimize W\n",
        "\n",
        "        loss_history = []\n",
        "        for it in range(num_iters):\n",
        "            X_batch = None\n",
        "            y_batch = None\n",
        "\n",
        "            # Sample batch_size elements in X_batch and y_batch\n",
        "            # X_batch shape is  (batch_size, d) and y_batch shape is (batch_size,)\n",
        "            # Hint: Use np.random.choice to generate indices\n",
        "            # YOUR CODE HERE\n",
        "            # raise NotImplementedError()\n",
        "            choice = np.random.choice(N, batch_size, replace=False)\n",
        "\n",
        "            X_batch = X[choice,:]\n",
        "            y_batch = y[choice]\n",
        "\n",
        "            # evaluate loss and gradient\n",
        "            loss, dW = self.loss(X_batch, y_batch, alpha)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            # perform parameter update\n",
        "            # Update the weights w using the gradient and the learning rate.\n",
        "            # YOUR CODE HERE\n",
        "            # raise NotImplementedError()\n",
        "            self.W -= learning_rate * dW\n",
        "\n",
        "            if verbose and it % 10000 == 0:\n",
        "                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
        "\n",
        "        return loss_history\n",
        "\n",
        "    def predict(self, X):\n",
        "        pass\n",
        "\n",
        "    def loss(self, X_batch, y_batch, reg):\n",
        "        pass\n",
        "\n",
        "class LinearSVM(LinearModel):\n",
        "    \"\"\" Softmax regression \"\"\"\n",
        "\n",
        "    def loss(self, X_batch, y_batch, alpha):\n",
        "        return svm_loss_vectorized(self.W, X_batch, y_batch, alpha)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - X: array of shape (N, D)\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: 1-dimensional array of length N, each element is an integer giving the predicted class\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n",
        "        if self.fit_intercept:\n",
        "            tmp = np.ones((len(X),1))\n",
        "            X = np.append(tmp, X, axis=1)\n",
        "\n",
        "        z = X.dot(self.W)\n",
        "        y_pred = np.argmax(z, axis=1)\n",
        "\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "344add9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "344add9f",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aee08beb636f046b69da3ada47961ae6",
          "grade": true,
          "grade_id": "cell-f1cb5669bcaed283",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "d2aa5174-873c-43cd-b5de-a2335886170f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration 0 / 75000: loss 1.999119\n",
            "iteration 10000 / 75000: loss 0.143852\n",
            "iteration 20000 / 75000: loss 0.143602\n",
            "iteration 30000 / 75000: loss 0.100846\n",
            "iteration 40000 / 75000: loss 0.115263\n",
            "iteration 50000 / 75000: loss 0.114839\n",
            "iteration 60000 / 75000: loss 0.049228\n",
            "iteration 70000 / 75000: loss 0.034964\n",
            "0.9733333333333334\n"
          ]
        }
      ],
      "source": [
        "model = LinearSVM(fit_intercept=True)\n",
        "model.train(X, y, num_iters=75000, batch_size=64, learning_rate=1e-3, verbose=True)\n",
        "pred = model.predict(X)\n",
        "model_accuracy = accuracy_score(y, pred)\n",
        "print(model_accuracy)\n",
        "assert model_accuracy > 0.97"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59873d46",
      "metadata": {
        "id": "59873d46"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
