{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3217513e",
   "metadata": {},
   "source": [
    "# MISA\n",
    "Alohan'ny mamerina dia avereno atao Run ny notebook iray manontolo. Ny fanaovana azy dia red√©marrena mihitsy ny kernel aloha (jereo menubar, safidio **Kernel$\\rightarrow$Restart Kernel and Run All Cells**).\n",
    "\n",
    "Izay misy hoe `YOUR CODE HERE` na `YOUR ANSWER HERE` ihany no fenoina. Afaka manampy cells vaovao raha ilaina. Aza adino ny mameno references eo ambany raha ilaina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c1437",
   "metadata": {},
   "source": [
    "## References\n",
    "Eto ilay references rehetra no apetraka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8575de52",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab42c33c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3d3c2e2a84ff92fa02220ae689dc414",
     "grade": false,
     "grade_id": "cell-38b4529793385ad1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.datasets import load_breast_cancer, load_diabetes\n",
    "\n",
    "\n",
    "def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5, error=1e-9):\n",
    "    \"\"\"\n",
    "    sample a few random elements and only return numerical\n",
    "    in this dimensions\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(num_checks):\n",
    "        ix = tuple([randrange(m) for m in x.shape])\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h  # increment by h\n",
    "        fxph = f(x)  # evaluate f(x + h)\n",
    "        x[ix] = oldval - h  # increment by h\n",
    "        fxmh = f(x)  # evaluate f(x - h)\n",
    "        x[ix] = oldval  # reset\n",
    "\n",
    "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "        grad_analytic = analytic_grad[ix]\n",
    "        rel_error = abs(grad_numerical - grad_analytic) / (\n",
    "            abs(grad_numerical) + abs(grad_analytic)\n",
    "        )\n",
    "        print(\n",
    "            \"numerical: %f analytic: %f, relative error: %e\"\n",
    "            % (grad_numerical, grad_analytic, rel_error)\n",
    "        )\n",
    "        assert rel_error < error\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec4298",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "731a18bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1159b5dc8c59570993ac39ae349c5037",
     "grade": false,
     "grade_id": "cell-3fb8ebd5eb97e246",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = load_diabetes()\n",
    "X_train1, y_train1 = data.data, data.target\n",
    "w1 = np.random.randn(X_train1.shape[1]) * 0.0001\n",
    "b1 = np.random.randn(1) * 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f07dea71",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "baf9b75ec047c1cdb887ecb2bc2043a1",
     "grade": false,
     "grade_id": "cell-5ffdf99ad7cdfd69",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mse_loss_naive(w, b, X, y, alpha=0):\n",
    "    \"\"\"\n",
    "    MSE loss function WITH FOR LOOPs\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - loss \n",
    "    - gradient with respect to weights w\n",
    "    - gradient with respect to bias b\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dw = np.zeros_like(w)\n",
    "    db = 0.0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "   \n",
    "    for i in range(len(y)):\n",
    "        loss += ((1 / len(y)) * ((w.T).dot(X[i]) + b - y[i]) ** 2)\n",
    "        db += 2 * (1 / len(y)) * ((w.T).dot(X[i]) + b - y[i])\n",
    "    loss+=alpha*(w.T).dot(w)+alpha*b**2\n",
    "    db+=2*alpha*b\n",
    "    for i in range (len(w)):\n",
    "        for j in range (len(y)):\n",
    "            dw[i] += 2 * (1 / len(y)) * (X[j][i]) * ((w.T).dot(X[j]) + b - y[j])\n",
    "        dw[i]+=2*(alpha)*w[i]\n",
    "    return loss, dw, np.array(db).reshape(1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddacc5d",
   "metadata": {},
   "source": [
    "## Naive Linear regression loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86cdcedc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c67f4964c919b07a15b9ca06dc2e8b1f",
     "grade": true,
     "grade_id": "cell-079e42b153b67a60",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss error :  5.005050726161976e-16\n",
      "Gradient check w\n",
      "numerical: -3.153316 analytic: -3.153317, relative error: 1.631872e-07\n",
      "numerical: -1.275043 analytic: -1.275044, relative error: 5.365633e-07\n",
      "numerical: -3.153316 analytic: -3.153317, relative error: 1.631872e-07\n",
      "numerical: -1.275043 analytic: -1.275044, relative error: 5.365633e-07\n",
      "numerical: -4.145418 analytic: -4.145418, relative error: 5.119551e-08\n",
      "numerical: -1.553189 analytic: -1.553189, relative error: 2.987241e-08\n",
      "numerical: -1.275043 analytic: -1.275044, relative error: 5.365633e-07\n",
      "numerical: -2.801914 analytic: -2.801913, relative error: 1.249387e-07\n",
      "numerical: -2.801914 analytic: -2.801913, relative error: 1.249387e-07\n",
      "numerical: -2.801914 analytic: -2.801913, relative error: 1.249387e-07\n",
      "numerical: -0.315455 analytic: -0.315453, relative error: 2.038005e-06\n",
      "numerical: -1.376395 analytic: -1.376394, relative error: 3.231708e-07\n",
      "numerical: -1.376395 analytic: -1.376394, relative error: 3.231708e-07\n",
      "numerical: -1.275043 analytic: -1.275044, relative error: 5.365633e-07\n",
      "numerical: 2.892059 analytic: 2.892060, relative error: 1.189958e-07\n",
      "Gradient check bias\n",
      "numerical: -304.266829 analytic: -304.266830, relative error: 1.324105e-09\n",
      "numerical: -304.266829 analytic: -304.266830, relative error: 1.324105e-09\n",
      "numerical: -304.266829 analytic: -304.266830, relative error: 1.324105e-09\n",
      "numerical: -304.266829 analytic: -304.266830, relative error: 1.324105e-09\n",
      "numerical: -304.266829 analytic: -304.266830, relative error: 1.324105e-09\n",
      "numerical: -304.266829 analytic: -304.266830, relative error: 1.324105e-09\n",
      "numerical: -304.266829 analytic: -304.266830, relative error: 1.324105e-09\n",
      "numerical: -304.266829 analytic: -304.266830, relative error: 1.324105e-09\n",
      "numerical: -304.266829 analytic: -304.266830, relative error: 1.324105e-09\n",
      "numerical: -304.266829 analytic: -304.266830, relative error: 1.324105e-09\n",
      "numerical: -304.266829 analytic: -304.266830, relative error: 1.324105e-09\n",
      "numerical: -304.266829 analytic: -304.266830, relative error: 1.324105e-09\n",
      "numerical: -304.266829 analytic: -304.266830, relative error: 1.324105e-09\n",
      "numerical: -304.266829 analytic: -304.266830, relative error: 1.324105e-09\n",
      "numerical: -304.266829 analytic: -304.266830, relative error: 1.324105e-09\n"
     ]
    }
   ],
   "source": [
    "loss, dw1, db1 = mse_loss_naive(w1, b1, X_train1, y_train1, alpha=0)\n",
    "\n",
    "sk_loss = mean_squared_error(X_train1 @ w1 + b1, y_train1)\n",
    "print(\"Loss error : \",rel_error(loss, sk_loss))\n",
    "assert rel_error(loss, sk_loss) < 1e-9\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: mse_loss_naive(w1, b1, X_train1, y_train1, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15,  error=1e-5)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: mse_loss_naive(w1, b1, X_train1, y_train1, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db438d3f",
   "metadata": {},
   "source": [
    "## Naive Ridge regression loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36465817",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ca5790893e5950341f7370bb3be167c",
     "grade": true,
     "grade_id": "cell-14a64cb1cfe70b98",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check w\n",
      "numerical: -0.315255 analytic: -0.315254, relative error: 1.959728e-06\n",
      "numerical: -3.153142 analytic: -3.153143, relative error: 1.605151e-07\n",
      "numerical: -2.802017 analytic: -2.802016, relative error: 1.184452e-07\n",
      "numerical: 2.891921 analytic: 2.891922, relative error: 1.166039e-07\n",
      "numerical: 2.891921 analytic: 2.891922, relative error: 1.166039e-07\n",
      "numerical: -2.802017 analytic: -2.802016, relative error: 1.184452e-07\n",
      "numerical: -1.275430 analytic: -1.275432, relative error: 5.472000e-07\n",
      "numerical: -4.296376 analytic: -4.296377, relative error: 3.835617e-08\n",
      "numerical: -3.234021 analytic: -3.234020, relative error: 1.687704e-07\n",
      "numerical: -2.802017 analytic: -2.802016, relative error: 1.184452e-07\n",
      "numerical: -0.315255 analytic: -0.315254, relative error: 1.959728e-06\n",
      "numerical: -1.376237 analytic: -1.376236, relative error: 3.622452e-07\n",
      "numerical: -2.802017 analytic: -2.802016, relative error: 1.184452e-07\n",
      "numerical: -1.376237 analytic: -1.376236, relative error: 3.622452e-07\n",
      "numerical: -1.553398 analytic: -1.553398, relative error: 3.319674e-08\n",
      "Gradient check bias\n",
      "numerical: -304.266691 analytic: -304.266692, relative error: 1.419138e-09\n",
      "numerical: -304.266691 analytic: -304.266692, relative error: 1.419138e-09\n",
      "numerical: -304.266691 analytic: -304.266692, relative error: 1.419138e-09\n",
      "numerical: -304.266691 analytic: -304.266692, relative error: 1.419138e-09\n",
      "numerical: -304.266691 analytic: -304.266692, relative error: 1.419138e-09\n",
      "numerical: -304.266691 analytic: -304.266692, relative error: 1.419138e-09\n",
      "numerical: -304.266691 analytic: -304.266692, relative error: 1.419138e-09\n",
      "numerical: -304.266691 analytic: -304.266692, relative error: 1.419138e-09\n",
      "numerical: -304.266691 analytic: -304.266692, relative error: 1.419138e-09\n",
      "numerical: -304.266691 analytic: -304.266692, relative error: 1.419138e-09\n",
      "numerical: -304.266691 analytic: -304.266692, relative error: 1.419138e-09\n",
      "numerical: -304.266691 analytic: -304.266692, relative error: 1.419138e-09\n",
      "numerical: -304.266691 analytic: -304.266692, relative error: 1.419138e-09\n",
      "numerical: -304.266691 analytic: -304.266692, relative error: 1.419138e-09\n",
      "numerical: -304.266691 analytic: -304.266692, relative error: 1.419138e-09\n"
     ]
    }
   ],
   "source": [
    "loss, dw1, db1 = mse_loss_naive(w1, b1, X_train1, y_train1, alpha=1)\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: mse_loss_naive(w1, b1, X_train1, y_train1, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15,  error=1e-5)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: mse_loss_naive(w1, b1, X_train1, y_train1, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c6b011f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21b3a09270262dbad9cb1c15c0fc69f8",
     "grade": false,
     "grade_id": "cell-1528a28f467d90c7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mse_loss_vectorized(w, b, X, y, alpha=0):\n",
    "    \"\"\"\n",
    "    MSE loss function WITHOUT FOR LOOPs\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - loss \n",
    "    - gradient with respect to weights w\n",
    "    - gradient with respect to bias b\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dw = np.zeros_like(w)\n",
    "    ones =np.ones((X.shape[0],1))\n",
    "    X = np.hstack((ones,X))\n",
    "    w = np.concatenate((np.array(b).reshape(1,),w),axis=0)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    v=np.dot(X,w)-y\n",
    "    loss = np.dot(v.T,v)/len(y)\n",
    "    deriv = 2/len(y)*(X.T).dot((X.dot(w)-y))\n",
    "    deriv += 2*(alpha)*w\n",
    "    dw = deriv[1:]\n",
    "    db = deriv[0]\n",
    "    \n",
    "    loss+=alpha*(w.T).dot(w)+alpha*b**2\n",
    "    \n",
    "    \n",
    "    return loss, dw, np.array(db).reshape(1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9bff00",
   "metadata": {},
   "source": [
    "## Vectorised Linear regression loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05dd3b4d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76d06d37e089b9fdf2c7ca7fa3241a61",
     "grade": true,
     "grade_id": "cell-41637ca21c8f938d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss error :  0.0\n",
      "Gradient check w\n",
      "numerical: -2.801913 analytic: -2.801913, relative error: 4.900373e-09\n",
      "numerical: -2.801913 analytic: -2.801913, relative error: 4.900373e-09\n",
      "numerical: -3.153317 analytic: -3.153317, relative error: 9.867661e-09\n",
      "numerical: -4.145418 analytic: -4.145418, relative error: 2.925575e-08\n",
      "numerical: -4.145418 analytic: -4.145418, relative error: 2.925575e-08\n",
      "numerical: -3.153317 analytic: -3.153317, relative error: 9.867661e-09\n",
      "numerical: -1.376394 analytic: -1.376394, relative error: 7.219510e-09\n",
      "numerical: -4.296088 analytic: -4.296088, relative error: 6.365543e-09\n",
      "numerical: -4.145418 analytic: -4.145418, relative error: 2.925575e-08\n",
      "numerical: -3.153317 analytic: -3.153317, relative error: 9.867661e-09\n",
      "numerical: -4.145418 analytic: -4.145418, relative error: 2.925575e-08\n",
      "numerical: -4.145418 analytic: -4.145418, relative error: 2.925575e-08\n",
      "numerical: -1.275044 analytic: -1.275044, relative error: 3.408044e-08\n",
      "numerical: -1.376394 analytic: -1.376394, relative error: 7.219510e-09\n",
      "numerical: -1.376394 analytic: -1.376394, relative error: 7.219510e-09\n",
      "Gradient check bias\n",
      "numerical: -304.266830 analytic: -304.266830, relative error: 1.704625e-10\n",
      "numerical: -304.266830 analytic: -304.266830, relative error: 1.704625e-10\n",
      "numerical: -304.266830 analytic: -304.266830, relative error: 1.704625e-10\n",
      "numerical: -304.266830 analytic: -304.266830, relative error: 1.704625e-10\n",
      "numerical: -304.266830 analytic: -304.266830, relative error: 1.704625e-10\n",
      "numerical: -304.266830 analytic: -304.266830, relative error: 1.704625e-10\n",
      "numerical: -304.266830 analytic: -304.266830, relative error: 1.704625e-10\n",
      "numerical: -304.266830 analytic: -304.266830, relative error: 1.704625e-10\n",
      "numerical: -304.266830 analytic: -304.266830, relative error: 1.704625e-10\n",
      "numerical: -304.266830 analytic: -304.266830, relative error: 1.704625e-10\n",
      "numerical: -304.266830 analytic: -304.266830, relative error: 1.704625e-10\n",
      "numerical: -304.266830 analytic: -304.266830, relative error: 1.704625e-10\n",
      "numerical: -304.266830 analytic: -304.266830, relative error: 1.704625e-10\n",
      "numerical: -304.266830 analytic: -304.266830, relative error: 1.704625e-10\n",
      "numerical: -304.266830 analytic: -304.266830, relative error: 1.704625e-10\n"
     ]
    }
   ],
   "source": [
    "loss, dw1, db1 = mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=0)\n",
    "\n",
    "sk_loss = mean_squared_error(X_train1 @ w1 + b1, y_train1)\n",
    "print(\"Loss error : \",rel_error(loss, sk_loss))\n",
    "assert rel_error(loss, sk_loss) < 1e-9\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15,  error=1e-5)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c8575f",
   "metadata": {},
   "source": [
    "## Vectorized ridge regression loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aa107f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89b351bee282f05c3f74c9541b3a825e",
     "grade": true,
     "grade_id": "cell-bb0ff99c19a85e0f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check w\n",
      "numerical: -1.376236 analytic: -1.376236, relative error: 3.181695e-08\n",
      "numerical: -2.802016 analytic: -2.802016, relative error: 1.138912e-08\n",
      "numerical: -3.234020 analytic: -3.234020, relative error: 2.815681e-08\n",
      "numerical: -1.275432 analytic: -1.275432, relative error: 2.327048e-08\n",
      "numerical: -3.153143 analytic: -3.153143, relative error: 1.254930e-08\n",
      "numerical: 2.891922 analytic: 2.891922, relative error: 9.194056e-09\n",
      "numerical: -3.234020 analytic: -3.234020, relative error: 2.815681e-08\n",
      "numerical: -3.234020 analytic: -3.234020, relative error: 2.815681e-08\n",
      "numerical: -1.275432 analytic: -1.275432, relative error: 2.327048e-08\n",
      "numerical: -3.234020 analytic: -3.234020, relative error: 2.815681e-08\n",
      "numerical: -0.315254 analytic: -0.315254, relative error: 5.973995e-08\n",
      "numerical: -2.802016 analytic: -2.802016, relative error: 1.138912e-08\n",
      "numerical: -1.376236 analytic: -1.376236, relative error: 3.181695e-08\n",
      "numerical: -2.802016 analytic: -2.802016, relative error: 1.138912e-08\n",
      "numerical: -4.145304 analytic: -4.145304, relative error: 3.312468e-08\n",
      "Gradient check bias\n",
      "numerical: -304.266553 analytic: -304.266692, relative error: 2.270990e-07\n",
      "numerical: -304.266553 analytic: -304.266692, relative error: 2.270990e-07\n",
      "numerical: -304.266553 analytic: -304.266692, relative error: 2.270990e-07\n",
      "numerical: -304.266553 analytic: -304.266692, relative error: 2.270990e-07\n",
      "numerical: -304.266553 analytic: -304.266692, relative error: 2.270990e-07\n",
      "numerical: -304.266553 analytic: -304.266692, relative error: 2.270990e-07\n",
      "numerical: -304.266553 analytic: -304.266692, relative error: 2.270990e-07\n",
      "numerical: -304.266553 analytic: -304.266692, relative error: 2.270990e-07\n",
      "numerical: -304.266553 analytic: -304.266692, relative error: 2.270990e-07\n",
      "numerical: -304.266553 analytic: -304.266692, relative error: 2.270990e-07\n",
      "numerical: -304.266553 analytic: -304.266692, relative error: 2.270990e-07\n",
      "numerical: -304.266553 analytic: -304.266692, relative error: 2.270990e-07\n",
      "numerical: -304.266553 analytic: -304.266692, relative error: 2.270990e-07\n",
      "numerical: -304.266553 analytic: -304.266692, relative error: 2.270990e-07\n",
      "numerical: -304.266553 analytic: -304.266692, relative error: 2.270990e-07\n"
     ]
    }
   ],
   "source": [
    "loss, dw1, db1 = mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=1)\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15,  error=1e-5)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7ae33",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb6d1eb4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd86b658f3ec113c00c2ee2e195f550f",
     "grade": false,
     "grade_id": "cell-434ec399cf8aeea7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X_train2, y_train2 = data.data, data.target\n",
    "w2 = np.random.randn(X_train2.shape[1]) * 0.0001\n",
    "b2 = np.random.randn(1) * 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97e5d16",
   "metadata": {},
   "source": [
    "# Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4690c331",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4357b941a04e6dc51a2683b7f4d91ed",
     "grade": false,
     "grade_id": "cell-6ec156568b0c6e29",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_loss_naive(w, b, X, y, alpha=0):\n",
    "    \"\"\"\n",
    "    log loss function WITH FOR LOOPs\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - loss \n",
    "    - gradient with respect to weights w\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dw = np.zeros_like(w)\n",
    "    db = 0.0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "     \n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        z = (w.T).dot(X[i])+b\n",
    "        A= sigmoid(z)\n",
    "        loss += (-y[i] * np.log(A) - (1 - y[i]) * np.log(1 - A))\n",
    "        \n",
    "        db += (A - y[i])\n",
    "        for k in range(len(w)):\n",
    "            dw[k]+=(A-y[i])*X[i][k]\n",
    "    loss =loss/len(y)\n",
    "    db = db/len(y)\n",
    "    dw= dw/len(y)\n",
    "    loss+=alpha*(w.T).dot(w)+alpha*b**2\n",
    "    db+=2*alpha*b\n",
    "    dw+=2*(alpha)*w\n",
    "    return loss, dw, np.array(db).reshape(1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f54b1bca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70d2045a5dac4c5a041dc8595cd5a39f",
     "grade": true,
     "grade_id": "cell-d3c078eb2449ee61",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss error :  7.70240915883959e-17\n",
      "Gradient check w\n",
      "numerical: 0.742211 analytic: 0.742211, relative error: 2.507711e-09\n",
      "numerical: 10.191149 analytic: 10.191150, relative error: 4.853932e-08\n",
      "numerical: -0.000187 analytic: -0.000187, relative error: 3.445378e-08\n",
      "numerical: 10.191149 analytic: 10.191150, relative error: 4.853932e-08\n",
      "numerical: 5.698381 analytic: 5.698383, relative error: 9.780162e-08\n",
      "numerical: 82.211410 analytic: 82.211914, relative error: 3.063762e-06\n",
      "numerical: 0.052169 analytic: 0.052169, relative error: 9.053593e-11\n",
      "numerical: 0.000737 analytic: 0.000737, relative error: 6.411997e-09\n",
      "numerical: 0.027995 analytic: 0.027995, relative error: 1.129124e-09\n",
      "numerical: 5.698381 analytic: 5.698383, relative error: 9.780162e-08\n",
      "numerical: 153.752250 analytic: 153.753996, relative error: 5.678853e-06\n",
      "numerical: -0.003445 analytic: -0.003445, relative error: 1.237264e-09\n",
      "numerical: -0.004819 analytic: -0.004819, relative error: 8.220889e-09\n",
      "numerical: -0.095170 analytic: -0.095170, relative error: 1.724633e-10\n",
      "numerical: -0.009087 analytic: -0.009087, relative error: 3.114050e-10\n",
      "Gradient check bias\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 1.097186e-10\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 1.097186e-10\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 1.097186e-10\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 1.097186e-10\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 1.097186e-10\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 1.097186e-10\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 1.097186e-10\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 1.097186e-10\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 1.097186e-10\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 1.097186e-10\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 1.097186e-10\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 1.097186e-10\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 1.097186e-10\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 1.097186e-10\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 1.097186e-10\n"
     ]
    }
   ],
   "source": [
    "y_pred_0 = sigmoid(X_train2 @ w2 + b2)\n",
    "y_pred = np.vstack([1-y_pred_0, y_pred_0]).T\n",
    "sk_loss = log_loss(y_train2, y_pred)\n",
    "\n",
    "loss, dw2, db2 = log_loss_naive(w2, b2, X_train2, y_train2, alpha=0)\n",
    "print(\"Loss error : \",rel_error(loss, sk_loss))\n",
    "assert rel_error(loss, sk_loss) < 1e-9\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w2: log_loss_naive(w2, b2, X_train2, y_train2, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w2, dw2, 15, error=1e-4)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b2: log_loss_naive(w2, b2, X_train2, y_train2, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b2, db2, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1e8516",
   "metadata": {},
   "source": [
    "# Naive with regulariztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79f6f9a6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb3c9584e2ca3502e5068d5c1ee2df96",
     "grade": true,
     "grade_id": "cell-b91805308ad91ec5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check w\n",
      "numerical: 0.011873 analytic: 0.011873, relative error: 4.699986e-10\n",
      "numerical: -0.009270 analytic: -0.009270, relative error: 2.489930e-10\n",
      "numerical: 0.000609 analytic: 0.000609, relative error: 1.909775e-08\n",
      "numerical: 82.211540 analytic: 82.212044, relative error: 3.063757e-06\n",
      "numerical: 2.545900 analytic: 2.545901, relative error: 1.185248e-07\n",
      "numerical: 0.022005 analytic: 0.022005, relative error: 1.028317e-09\n",
      "numerical: 0.288017 analytic: 0.288017, relative error: 3.719337e-09\n",
      "numerical: 0.001295 analytic: 0.001295, relative error: 1.175466e-08\n",
      "numerical: 0.288017 analytic: 0.288017, relative error: 3.719337e-09\n",
      "numerical: 0.018397 analytic: 0.018397, relative error: 1.410402e-10\n",
      "numerical: -0.000743 analytic: -0.000743, relative error: 7.898853e-09\n",
      "numerical: -0.095314 analytic: -0.095314, relative error: 1.645748e-10\n",
      "numerical: -0.003481 analytic: -0.003481, relative error: 1.341357e-09\n",
      "numerical: 153.752628 analytic: 153.754374, relative error: 5.678840e-06\n",
      "numerical: 0.742502 analytic: 0.742502, relative error: 2.507273e-09\n",
      "Gradient check bias\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 1.187183e-10\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 1.187183e-10\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 1.187183e-10\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 1.187183e-10\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 1.187183e-10\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 1.187183e-10\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 1.187183e-10\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 1.187183e-10\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 1.187183e-10\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 1.187183e-10\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 1.187183e-10\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 1.187183e-10\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 1.187183e-10\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 1.187183e-10\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 1.187183e-10\n"
     ]
    }
   ],
   "source": [
    "loss, dw2, db2 = log_loss_naive(w2, b2, X_train2, y_train2, alpha=1)\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w2: log_loss_naive(w2, b2, X_train2, y_train2, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f, w2, dw2, 15, error=1e-4)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b2: log_loss_naive(w2, b2, X_train2, y_train2, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b2, db2, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31564092",
   "metadata": {},
   "source": [
    "# Vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc5084e9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f452aaa24d69e2257c0851d750fd4aa",
     "grade": false,
     "grade_id": "cell-a96e9a6d51919ffc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_loss_vectorized(w, b,X, y, alpha=0):\n",
    "    \"\"\"\n",
    "    log loss function WITHOUT FOR LOOPs\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - loss \n",
    "    - gradient with respect to weights w\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dw = np.zeros_like(w)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    Z = X.dot(w)+ b \n",
    "    A= sigmoid(Z)\n",
    "    loss =1/len(y) * np.sum(-y * np.log(A)-(1-y)*np.log(1-A))\n",
    "    dw=1/len(y)*np.dot(X.T,A-y)\n",
    "    db = 1/len(y)*np.sum(A-y)\n",
    "    loss+=alpha*(w.T).dot(w)+alpha*b**2\n",
    "    db+=2*alpha*b\n",
    "    dw+=2*(alpha)*w\n",
    "    return loss, dw, np.array(db).reshape(1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c425e768",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f415c75f4653f455eeeccba9f7ad1676",
     "grade": true,
     "grade_id": "cell-ca14e49ada130789",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss error :  0.0\n",
      "Gradient check w\n",
      "numerical: 0.011895 analytic: 0.011895, relative error: 2.480508e-11\n",
      "numerical: -0.095170 analytic: -0.095170, relative error: 2.664265e-11\n",
      "numerical: 82.211410 analytic: 82.211914, relative error: 3.063762e-06\n",
      "numerical: -0.000187 analytic: -0.000187, relative error: 9.977569e-09\n",
      "numerical: 0.018160 analytic: 0.018160, relative error: 8.233621e-11\n",
      "numerical: -0.004819 analytic: -0.004819, relative error: 4.184906e-10\n",
      "numerical: 82.211410 analytic: 82.211914, relative error: 3.063762e-06\n",
      "numerical: 153.752250 analytic: 153.753996, relative error: 5.678853e-06\n",
      "numerical: 0.288417 analytic: 0.288417, relative error: 3.644901e-09\n",
      "numerical: 82.211410 analytic: 82.211914, relative error: 3.063762e-06\n",
      "numerical: -0.004716 analytic: -0.004716, relative error: 3.106722e-10\n",
      "numerical: 0.742211 analytic: 0.742211, relative error: 2.507712e-09\n",
      "numerical: 0.011895 analytic: 0.011895, relative error: 2.480508e-11\n",
      "numerical: 0.021882 analytic: 0.021882, relative error: 1.319556e-10\n",
      "numerical: 0.011895 analytic: 0.011895, relative error: 2.480508e-11\n",
      "Gradient check bias\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 3.796869e-11\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 3.796869e-11\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 3.796869e-11\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 3.796869e-11\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 3.796869e-11\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 3.796869e-11\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 3.796869e-11\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 3.796869e-11\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 3.796869e-11\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 3.796869e-11\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 3.796869e-11\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 3.796869e-11\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 3.796869e-11\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 3.796869e-11\n",
      "numerical: -0.075174 analytic: -0.075174, relative error: 3.796869e-11\n"
     ]
    }
   ],
   "source": [
    "y_pred_0 = sigmoid(X_train2 @ w2 + b2)\n",
    "y_pred = np.vstack([1-y_pred_0, y_pred_0]).T\n",
    "sk_loss = log_loss(y_train2, y_pred)\n",
    "\n",
    "loss, dw2, db2 = log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=0)\n",
    "print(\"Loss error : \",rel_error(loss, sk_loss))\n",
    "assert rel_error(loss, sk_loss) < 1e-9\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w2: log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w2, dw2, 15, error=1e-4)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b2: log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b2, db2, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7be5da",
   "metadata": {},
   "source": [
    "# Vectorized with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4e09127",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53e97adc2666d529eff9ba1abcd3756a",
     "grade": true,
     "grade_id": "cell-bce082fccf8b7057",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check w\n",
      "numerical: -0.003481 analytic: -0.003481, relative error: 2.531082e-10\n",
      "numerical: 0.049958 analytic: 0.049958, relative error: 3.277218e-13\n",
      "numerical: -0.000096 analytic: -0.000096, relative error: 1.795359e-08\n",
      "numerical: 0.375425 analytic: 0.375425, relative error: 1.152036e-10\n",
      "numerical: 0.018397 analytic: 0.018397, relative error: 1.607025e-10\n",
      "numerical: 153.752628 analytic: 153.754374, relative error: 5.678840e-06\n",
      "numerical: -0.095314 analytic: -0.095314, relative error: 1.897485e-11\n",
      "numerical: 82.211540 analytic: 82.212044, relative error: 3.063757e-06\n",
      "numerical: -0.001312 analytic: -0.001312, relative error: 5.813581e-10\n",
      "numerical: 0.288017 analytic: 0.288017, relative error: 3.651879e-09\n",
      "numerical: 0.000609 analytic: 0.000609, relative error: 3.674196e-09\n",
      "numerical: 0.288017 analytic: 0.288017, relative error: 3.651879e-09\n",
      "numerical: -0.541181 analytic: -0.541181, relative error: 3.648502e-09\n",
      "numerical: -0.004976 analytic: -0.004976, relative error: 3.671134e-10\n",
      "numerical: 0.375425 analytic: 0.375425, relative error: 1.152036e-10\n",
      "Gradient check bias\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 2.860986e-11\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 2.860986e-11\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 2.860986e-11\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 2.860986e-11\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 2.860986e-11\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 2.860986e-11\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 2.860986e-11\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 2.860986e-11\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 2.860986e-11\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 2.860986e-11\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 2.860986e-11\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 2.860986e-11\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 2.860986e-11\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 2.860986e-11\n",
      "numerical: -0.075357 analytic: -0.075357, relative error: 2.860986e-11\n"
     ]
    }
   ],
   "source": [
    "loss, dw2, db2 = log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=1)\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w2: log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f, w2, dw2, 15, error=1e-4)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b2: log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b2, db2, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06403ea0",
   "metadata": {},
   "source": [
    "# Gradient descent for Linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5039af1c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a1783db1a1f1898028260238567bed7",
     "grade": false,
     "grade_id": "cell-485e52c0efd4f4a9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearModel():\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, alpha=0, num_iters=100, batch_size=200, verbose=False):\n",
    "        N, d = X.shape\n",
    "        \n",
    "        if self.w is None: # Initialization\n",
    "            self.w = 0.001 * np.random.randn(d)\n",
    "            self.b = 0.0\n",
    "\n",
    "        # Run stochastic gradient descent to optimize w\n",
    "        \n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "                                                               \n",
    "            # Sample batch_size elements in X_batch and y_batch\n",
    "            # X_batch shape is  (batch_size, d) and y_batch shape is (batch_size,)                                                                                          \n",
    "            # Hint: Use np.random.choice to generate indices\n",
    "            # YOUR CODE HERE\n",
    "            index = np.random.choice(N,batch_size,replace=False)\n",
    "            X_batch = np.array([X[i] for i in index])\n",
    "            y_batch = np.array([y[i] for i in index])\n",
    "            \n",
    "            \n",
    "            # evaluate loss and gradient\n",
    "            loss, dw, db = self.loss(X_batch, y_batch, alpha)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            # perform parameter update                                                                \n",
    "            # Update the weights w and bias b using the gradient and the learning rate.          \n",
    "            # YOUR CODE HERE\n",
    "            self.w= self.w- learning_rate * dw \n",
    "            self.b = self.b- learning_rate * db \n",
    "                    \n",
    "            if verbose and it % 10000 == 0:\n",
    "                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
    "                \n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        pass\n",
    "\n",
    "class LinearRegressor(LinearModel):\n",
    "    \"\"\" Linear regression \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, alpha):\n",
    "        return mse_loss_vectorized(self.w, self.b, X_batch, y_batch, alpha)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # YOUR CODE HERE\n",
    "        y = X.dot(self.w) + self.b\n",
    "        return y\n",
    "\n",
    "        \n",
    "class LogisticRegressor(LinearModel):\n",
    "    \"\"\" Linear regression \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, alpha):\n",
    "        return log_loss_vectorized(self.w, self.b, X_batch, y_batch, alpha)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Return prediction labels vector of 0 or 1 \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        y = X.dot(self.w) + self.b\n",
    "        r = np.zeros_like(y)\n",
    "        for i in range(len(y)):\n",
    "            r[i] = 1 if y[i]>0 else 0\n",
    "        return r\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa0aa79",
   "metadata": {},
   "source": [
    "## Linear regression with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86925e00",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30f1c271b16efe670214663c10743937",
     "grade": true,
     "grade_id": "cell-92f36a3b387a4277",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 75000: loss 27084.531224\n",
      "iteration 10000 / 75000: loss 3066.402594\n",
      "iteration 20000 / 75000: loss 2812.985126\n",
      "iteration 30000 / 75000: loss 2943.337115\n",
      "iteration 40000 / 75000: loss 2548.353319\n",
      "iteration 50000 / 75000: loss 3001.407949\n",
      "iteration 60000 / 75000: loss 2525.551435\n",
      "iteration 70000 / 75000: loss 4153.144433\n",
      "MSE scikit-learn: 2859.6963475867506\n",
      "MSE gradient descent model : 2884.3536081730335\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "sk_model = LinearRegression(fit_intercept=True)\n",
    "sk_model.fit(X_train1, y_train1)\n",
    "sk_pred = sk_model.predict(X_train1)\n",
    "sk_mse = mean_squared_error(sk_pred, y_train1)\n",
    "\n",
    "model = LinearRegressor()\n",
    "model.train(X_train1, y_train1, num_iters=75000, batch_size=64, learning_rate=1e-2, verbose=True)\n",
    "pred = model.predict(X_train1)\n",
    "mse = mean_squared_error(pred, y_train1)\n",
    "\n",
    "print(\"MSE scikit-learn:\", sk_mse)\n",
    "print(\"MSE gradient descent model :\", mse)\n",
    "assert mse - sk_mse < 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c54d8",
   "metadata": {},
   "source": [
    "## Logistc regression with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9219f422",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6196155f76f4eded31fefce67a76d60",
     "grade": true,
     "grade_id": "cell-925a2ddb5c1ba7ff",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 75000: loss 0.692680\n",
      "iteration 10000 / 75000: loss 0.094124\n",
      "iteration 20000 / 75000: loss 0.052587\n",
      "iteration 30000 / 75000: loss 0.102940\n",
      "iteration 40000 / 75000: loss 0.160897\n",
      "iteration 50000 / 75000: loss 0.083378\n",
      "iteration 60000 / 75000: loss 0.120880\n",
      "iteration 70000 / 75000: loss 0.023333\n",
      "Log-loss scikit-learn: 0.44341928598210933\n",
      "Log-loss gradiet descent model : 0.44341928598210933\n",
      "Error : 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train2 = scaler.fit_transform(X_train2)\n",
    "\n",
    "sk_model = LogisticRegression(fit_intercept=True)\n",
    "sk_model.fit(X_train2, y_train2)\n",
    "sk_pred = sk_model.predict(X_train2)\n",
    "sk_log_loss = log_loss(sk_pred, y_train2)\n",
    "\n",
    "model = LogisticRegressor()\n",
    "model.train(X_train2, y_train2, num_iters=75000, batch_size=64, learning_rate=1e-3, verbose=True)\n",
    "pred = model.predict(X_train2)\n",
    "model_log_loss = log_loss(pred, y_train2)\n",
    "\n",
    "print(\"Log-loss scikit-learn:\", sk_log_loss)\n",
    "print(\"Log-loss gradiet descent model :\", model_log_loss)\n",
    "print(\"Error :\", rel_error(sk_log_loss, model_log_loss))\n",
    "assert rel_error(sk_log_loss, model_log_loss) < 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f7431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
